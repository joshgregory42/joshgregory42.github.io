<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.54">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Josh Gregory">
<meta name="dcterms.date" content="2024-06-07">
<meta name="description" content="My notes from the GPT and Chill playlist">

<title>GPT and Chill Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-TXK1ED55CF"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-TXK1ED55CF', { 'anonymize_ip': true});
</script>
<meta name="mermaid-theme" content="neutral">
<script src="../../site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="../../site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="../../site_libs/quarto-diagram/mermaid.css" rel="stylesheet">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Josh Gregory</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../projects/index.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../resume_jag.pdf"> 
<span class="menu-text">Resume</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/josh-a-gregory"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/joshgregory42"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/josh_a_gregory"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page-left">
      <h1 class="title">GPT and Chill Notes</h1>
                  <div>
        <div class="description">
          My notes from the GPT and Chill playlist
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Notes</div>
                <div class="quarto-category">Deep Learning</div>
                <div class="quarto-category">Machine Learning</div>
                <div class="quarto-category">Artificial Intelligence</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta column-page-left">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://joshgregory.github.io/">Josh Gregory</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 7, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#how-neural-networks-learn-gradient-descent" id="toc-how-neural-networks-learn-gradient-descent" class="nav-link active" data-scroll-target="#how-neural-networks-learn-gradient-descent">How Neural Networks Learn (Gradient Descent)</a>
  <ul class="collapse">
  <li><a href="#multivariate-gradient-descent" id="toc-multivariate-gradient-descent" class="nav-link" data-scroll-target="#multivariate-gradient-descent">Multivariate Gradient Descent</a></li>
  </ul></li>
  <li><a href="#linear-regression-full-explanation-coding-problem" id="toc-linear-regression-full-explanation-coding-problem" class="nav-link" data-scroll-target="#linear-regression-full-explanation-coding-problem">Linear Regression: Full Explanation &amp; Coding Problem</a>
  <ul class="collapse">
  <li><a href="#the-regression-part" id="toc-the-regression-part" class="nav-link" data-scroll-target="#the-regression-part">The “Regression” part</a>
  <ul class="collapse">
  <li><a href="#classification" id="toc-classification" class="nav-link" data-scroll-target="#classification">Classification</a></li>
  <li><a href="#regression" id="toc-regression" class="nav-link" data-scroll-target="#regression">Regression</a></li>
  </ul></li>
  <li><a href="#the-linear-part" id="toc-the-linear-part" class="nav-link" data-scroll-target="#the-linear-part">The “Linear” part</a></li>
  <li><a href="#implementation" id="toc-implementation" class="nav-link" data-scroll-target="#implementation">Implementation</a>
  <ul class="collapse">
  <li><a href="#solution---first-problem" id="toc-solution---first-problem" class="nav-link" data-scroll-target="#solution---first-problem">Solution - First Problem</a></li>
  <li><a href="#solution---second-problem" id="toc-solution---second-problem" class="nav-link" data-scroll-target="#solution---second-problem">Solution - Second Problem</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#neural-networks-in-10-minutes---end-to-end-explanation" id="toc-neural-networks-in-10-minutes---end-to-end-explanation" class="nav-link" data-scroll-target="#neural-networks-in-10-minutes---end-to-end-explanation">Neural Networks in 10 Minutes - End to End Explanation</a>
  <ul class="collapse">
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link" data-scroll-target="#linear-regression">Linear Regression</a></li>
  <li><a href="#hidden-layer" id="toc-hidden-layer" class="nav-link" data-scroll-target="#hidden-layer">Hidden Layer</a></li>
  <li><a href="#output-layer" id="toc-output-layer" class="nav-link" data-scroll-target="#output-layer">Output Layer</a></li>
  </ul></li>
  <li><a href="#intro-to-pytorch.-forget-tensorflow" id="toc-intro-to-pytorch.-forget-tensorflow" class="nav-link" data-scroll-target="#intro-to-pytorch.-forget-tensorflow">Intro to PyTorch. Forget TensorFlow</a>
  <ul class="collapse">
  <li><a href="#defining-neural-network-models-in-pytorch" id="toc-defining-neural-network-models-in-pytorch" class="nav-link" data-scroll-target="#defining-neural-network-models-in-pytorch">Defining Neural Network Models in PyTorch</a>
  <ul class="collapse">
  <li><a href="#module-class" id="toc-module-class" class="nav-link" data-scroll-target="#module-class"><code>Module</code> Class</a></li>
  <li><a href="#linear-class" id="toc-linear-class" class="nav-link" data-scroll-target="#linear-class"><code>Linear</code> Class</a></li>
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example">Example</a></li>
  </ul></li>
  <li><a href="#coding-problem-solution" id="toc-coding-problem-solution" class="nav-link" data-scroll-target="#coding-problem-solution">Coding Problem Solution</a></li>
  </ul></li>
  <li><a href="#dropout" id="toc-dropout" class="nav-link" data-scroll-target="#dropout">Dropout</a>
  <ul class="collapse">
  <li><a href="#coding-problem-solution-1" id="toc-coding-problem-solution-1" class="nav-link" data-scroll-target="#coding-problem-solution-1">Coding Problem Solution</a></li>
  </ul></li>
  <li><a href="#training-neural-networks-in-pytorch" id="toc-training-neural-networks-in-pytorch" class="nav-link" data-scroll-target="#training-neural-networks-in-pytorch">Training Neural Networks in PyTorch</a></li>
  <li><a href="#intro-to-nlp" id="toc-intro-to-nlp" class="nav-link" data-scroll-target="#intro-to-nlp">Intro to NLP</a>
  <ul class="collapse">
  <li><a href="#the-setup" id="toc-the-setup" class="nav-link" data-scroll-target="#the-setup">The Setup</a></li>
  </ul></li>
  <li><a href="#machine-learning-on-tweets" id="toc-machine-learning-on-tweets" class="nav-link" data-scroll-target="#machine-learning-on-tweets">Machine Learning on Tweets</a>
  <ul class="collapse">
  <li><a href="#model-architecture" id="toc-model-architecture" class="nav-link" data-scroll-target="#model-architecture">Model Architecture</a>
  <ul class="collapse">
  <li><a href="#lookup-table" id="toc-lookup-table" class="nav-link" data-scroll-target="#lookup-table">Lookup Table</a></li>
  </ul></li>
  <li><a href="#coding-problem-solution-2" id="toc-coding-problem-solution-2" class="nav-link" data-scroll-target="#coding-problem-solution-2">Coding Problem Solution</a></li>
  </ul></li>
  <li><a href="#illustrated-guide-to-attention---how-chatgpt-reads" id="toc-illustrated-guide-to-attention---how-chatgpt-reads" class="nav-link" data-scroll-target="#illustrated-guide-to-attention---how-chatgpt-reads">Illustrated Guide to Attention - How ChatGPT Reads</a>
  <ul class="collapse">
  <li><a href="#the-mathematical-explanation" id="toc-the-mathematical-explanation" class="nav-link" data-scroll-target="#the-mathematical-explanation">The Mathematical Explanation</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block column-page-left" id="quarto-document-content">





<blockquote class="blockquote">
<p>You can view the entire playlist <a href="https://www.gptandchill.ai/codingproblems">here</a></p>
</blockquote>
<section id="how-neural-networks-learn-gradient-descent" class="level1">
<h1>How Neural Networks Learn (Gradient Descent)</h1>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/bbYdqd6wemI" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Let’s assume that we have a model that takes two inputs: <span class="math inline">\(x_1\)</span> (your weight at age 10) and <span class="math inline">\(x_2\)</span> (your height at age 10) and outputs a prediction <span class="math inline">\(y\)</span> for your final height as an adult.</p>
<p>Gradient descent is just a way to minimize a function. That’s it. So if I have a function like <span class="math inline">\(y = x^2\)</span>, we can minimize this pretty easily by taking the derivative and setting it equal to zero:</p>
<p><span class="math display">\[
\begin{align*}
y &amp;= x^2\\
y' &amp;= 2x\\
2x &amp;= 0\\
x &amp;= 0
\end{align*}
\]</span></p>
<p>which corresponds to the minimum of <span class="math inline">\(y = x^2\)</span>. But that’s only for one input and one output. We usually have multiple inputs (like here it’s height and weight). So how can we handle that? What if we can’t even take the derivative?</p>
<p>Let’s back up and ask a more important question first. Why do we even <strong>need</strong> to minimize a function to train a model?</p>
<p>The function we’re minimizing is the <strong>error function</strong>. Aka the <strong>loss</strong> or the <strong>cost function</strong>, it is the error between the model’s prediction and the <strong>label</strong> (the actual value). In our example here, the error would be the difference between what the model <em>thinks</em> the final height should be given a 10 year-old’s weight (<span class="math inline">\(x_1\)</span>) and height (<span class="math inline">\(x_2\)</span>), and what the height <em>actually</em> is (the label). There are different ways we can calculate the error, but here we could do something simple like the absolute difference:</p>
<p><span class="math display">\[
\begin{equation*}
\text{Error} \approx \left| \text{Prediction} - \text{label} \right|
\end{equation*}
\]</span> Let’s build some intuition. Let’s return to <span class="math inline">\(y = x^2\)</span>. Gradient descent is typically called “stochastic gradient descent” (SGD), with “stochastic” referring to a random guess for the minimum. Let’s apply that here to <span class="math inline">\(y = x^2\)</span>. Let’s have an initial guess of <span class="math inline">\(x=3\)</span> (remember that we know the actual minimum is <span class="math inline">\(x = 0\)</span>). Let’s look at the slope of the tangent line at <span class="math inline">\(x = 3\)</span>, which is the same thing as the <em>derivative</em> or the <em>gradient</em>. Here it is positive:</p>
<p><span class="math display">\[
\begin{align*}
f' &amp;= 2 x\\
f' (x = 3) &amp;= (2)(3) = 6
\end{align*}
\]</span></p>
<p>which means that the function is <strong>increasing</strong>. If we want to minimize a function, we want to go in the opposite direction. We want to find where there is no change (<span class="math inline">\(f'=0\)</span>), assuming the boundaries of the function do not contain the minima (that’s an easy check though). So our new guess:</p>
<p><span class="math display">\[
\text{New guess} = \text{old guess} - \left( \text{slope} \right) \left( \text{step size} \right)
\]</span> Let’s call the step size <span class="math inline">\(\alpha\)</span>. So more succinctly:</p>
<p><span class="math display">\[
\text{Guess} -= \text{slope} \cdot \alpha
\]</span> In Python, we could write a class that implements this:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GradDescent:</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_minimizer(<span class="va">self</span>, iterations: <span class="bu">int</span>, learning_rate: <span class="bu">float</span>, guess: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, iterations):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>            guess <span class="op">-=</span> (<span class="dv">2</span><span class="op">*</span>guess)<span class="op">*</span>learning_rate</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="bu">round</span>(guess, <span class="dv">5</span>)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>sol <span class="op">=</span> GradDescent()</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> sol.get_minimizer(iterations<span class="op">=</span><span class="dv">10</span>, learning_rate<span class="op">=</span><span class="fl">0.01</span>, guess<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="multivariate-gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="multivariate-gradient-descent">Multivariate Gradient Descent</h2>
<p>Let’s say that we have a new function related with two variables:</p>
<p><span class="math display">\[
f(x,y) = x^2 + y^2
\]</span> Now we have an actual gradient, that is</p>
<p><span class="math display">\[
\begin{align*}
\nabla f(x,y) &amp;= \left \langle \frac{\partial f}{ \partial x }, \frac{\partial f}{ \partial y } \right \rangle\\
&amp;= \left \langle 2x, 2y \right \rangle
\end{align*}
\]</span> So now we have two values to update (one for each component, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>), which means that we also need two initial guesses for <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. So our updating algorithm now looks like this for <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[
\begin{align*}
\text{guess} &amp;-= \frac{\partial f}{ \partial x } \bigg|_{x=x_{\text{guess}}} \cdot \alpha\\
&amp;-= 2 x \bigg|_{x=x_{\text{guess}}} \alpha
\end{align*}
\]</span> and for <span class="math inline">\(y\)</span>: <span class="math display">\[
\begin{align*}
\text{guess} &amp;-= \frac{\partial f}{ \partial y } \bigg|_{y=y_{\text{guess}}} \cdot \alpha\\
&amp;-= 2 y \bigg|_{y=y_{\text{guess}}} \alpha
\end{align*}
\]</span> Note that the partial derivatives being the same here is purely a coincidence and is only because <span class="math inline">\(f(x,y)\)</span> is defined as <span class="math inline">\(f(x, y)=x^2 + y^2\)</span>. If we had changed it to something like <span class="math inline">\(f(x, y) = x^3 - y^{1/2}\)</span> or something like that, then the partial derivatives would obviously not be equal.</p>
<p>If you do the calculus, you find that the minimum of <span class="math inline">\(f(x,y)\)</span> is at (0, 0). Here’s the code verifying that:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GradDescentMulti:</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_min(<span class="va">self</span>, iterations: <span class="bu">int</span>, learning_rate: <span class="bu">float</span>, guess_x: <span class="bu">int</span>, guess_y: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, iterations):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>            guess_x <span class="op">-=</span> <span class="dv">2</span><span class="op">*</span>guess_x <span class="op">*</span> learning_rate</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>            guess_y <span class="op">-=</span> <span class="dv">2</span><span class="op">*</span>guess_y <span class="op">*</span> learning_rate</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>            guess_x <span class="op">=</span> <span class="bu">round</span>(guess_x, <span class="dv">5</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>            guess_y <span class="op">=</span> <span class="bu">round</span>(guess_y, <span class="dv">5</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> guess_x, guess_y</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>gd <span class="op">=</span> GradDescentMulti()</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>x_min, y_min <span class="op">=</span> gd.get_min(iterations<span class="op">=</span><span class="dv">1000</span>, learning_rate<span class="op">=</span><span class="fl">0.1</span>, guess_x <span class="op">=</span> <span class="dv">2</span>, guess_y <span class="op">=</span> <span class="dv">5</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'x_min: </span><span class="sc">{</span>x_min<span class="sc">}</span><span class="ss">, y_min: </span><span class="sc">{</span>y_min<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">x_min:</span> 2e-05, y_min: 2e-05</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>which within rounding precision is equal to zero for both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p>
</section>
</section>
<section id="linear-regression-full-explanation-coding-problem" class="level1">
<h1>Linear Regression: Full Explanation &amp; Coding Problem</h1>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/K9xTjTP0vVw" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Linear regression is the foundation of neural networks and it behind many of the recent advancements in ChatGPT, self-driving, and deepfakes.</p>
<section id="the-regression-part" class="level2">
<h2 class="anchored" data-anchor-id="the-regression-part">The “Regression” part</h2>
<section id="classification" class="level3">
<h3 class="anchored" data-anchor-id="classification">Classification</h3>
<p>Let’s start with a more intuitive example, which is actually the opposite of regression and it’s <strong>classification</strong>. As an example, let’s say that we’re building a model that detects whether someone has diabetes. There are two (simplified) outcomes here: the person has diabetes or they don’t. So we’re classifying our input (maybe an image) into two classification “buckets”.</p>
<p>A more complicated example would be an object-detection model. So you give your model an input image and it tells you whether it’s a cat, a dog, an apple, or an orange. This is the same as the diabetes example, just with more classification buckets.</p>
</section>
<section id="regression" class="level3">
<h3 class="anchored" data-anchor-id="regression">Regression</h3>
<p>For regression, the output is a number that is real (between negative and positive infinity). So if you want a model that will predict someone’s final height given their current height, current weight, how tall their parent’s are (and possibly any other relevant features), the output exists on some scale (so a number). Unlike classification, regression returns a number (for our example, something like 70, for 70 inches in height which is 5’10”) instead of a category (like “orange”).</p>
</section>
</section>
<section id="the-linear-part" class="level2">
<h2 class="anchored" data-anchor-id="the-linear-part">The “Linear” part</h2>
<p>When we’re building our model, we want it to make some kind of prediction given some piece of information we provide. The <strong>linear</strong> part here is saying that the relationship between the input to the model (like the current height, current weight, parent’s height, etc.) and the output (final height) is linear, so it looks something like this:</p>
<p><span class="math display">\[
h(x, y, z) = w_1 x + w_2 y + w_3 z + b
\]</span> where</p>
<p><span class="math display">\[
\begin{align*}
h &amp;\text{ is the final height}\\
x &amp;\text{ is the current weight}\\
y &amp;\text{ is the current height}\\
z &amp;\text{ is the parents' height}\\
w_n &amp;\text{ is the model weight}\\
b &amp;\text{ is the model bias}\\
\end{align*}
\]</span> The “linear” part is from <span class="math inline">\(h(x, y, z)\)</span> being a linear equation. So there isn’t anythng like <span class="math inline">\(w_1 x^4\)</span> or <span class="math inline">\(w_2 \cos{y}\)</span>. This means that, in short, we have a <em>really fancy version of</em> <span class="math inline">\(y = mx + b\)</span>, but <span class="math inline">\(m\)</span> and <span class="math inline">\(x\)</span> can have an arbitrary number of components or inputs.</p>
<p>Then, during training via gradient descent, the model will improve <span class="math inline">\(w_1, w_2, w_3, \ldots, w_n\)</span> and <span class="math inline">\(b\)</span> over some fixed number of iterations until the model is a pretty good way to predict someone’s final height given their current height, weight, and that of their parents.</p>
<p>The pseudocode for this would look something like:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> num_iterations:</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  get_model_prediction()</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  get_error() <span class="co"># want this to approach zero</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  get_derivatives()</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  update_weights()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Focusing a bit more on the <code>get_error()</code> part, there are many different ways we can determine the error in a model, but one of the most common ones is called <strong>Mean Squared Error</strong> (MSE):</p>
<p><span class="math display">\[
\text{MSE} = \sum_{i = 1}^{N} \frac{\left( \text{prediction}_i - \text{label}_i \right)^2}{N}
\]</span> where <span class="math inline">\(N\)</span> is the number of training examples, and recall that the label is defined as the “true” answer that we’re comparing our model’s prediction against.</p>
<p>A common question to ask here is why don’t we use the absolute value instead of squaring to get our error? This is because the derivative won’t exist somewhere. If we look at the most basic absolute value function, <span class="math inline">\(y = |x|\)</span>, we get something like this:</p>
<div id="27bf0852" class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-2-output-1.png" width="585" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>which has an undefined derivative at the origin (technically, this makes the absolute value function “non-differentiable”). In other words, for <span class="math inline">\(y = |x|\)</span>, <span class="math inline">\(y'\)</span> doesn’t exist at <span class="math inline">\(x=0\)</span>, which would break the algorithm that we’ve developed so far. Squaring solves this problem and still gives us the same general idea as the absolute value, that is, tells us how good/bad the error is.</p>
<p>Implementing this in code, we use vectors and matrices:</p>
<p><span class="math display">\[
\begin{bmatrix}
x &amp; y &amp; z
\end{bmatrix}
\begin{bmatrix}
w_1\\
w_2\\
w_3
\end{bmatrix}
= w_1 x + w_2 y + w_3 z = \text{model prediction}
\]</span> But what if we had many people? Let’s look at what this would look like if we had three people: <span class="math display">\[
\begin{bmatrix}
x_1 &amp; y_1 &amp; z_1\\
x_2 &amp; y_2 &amp; z_2\\
x_3 &amp; y_3 &amp; z_3
\end{bmatrix}
\begin{bmatrix}
w_1\\
w_2\\
w_3
\end{bmatrix}
=
\begin{bmatrix}
w_1 x_1 + w_2 y_1 + w_3 z_1\\
w_1 x_2 + w_2 y_2 + w_3 z_2\\
w_1 x_3 + w_2 y_3 + w_3 z_3\\
\end{bmatrix}
\]</span> where each row is the model’s prediction for each person (i.e.&nbsp;the first row is the model’s prediction for the first person, second row is the prediction for the second person, etc.). An important note here is that at this point we’re just doing matrix multiplication. The main advantage of doing this in matrices vs.&nbsp;a loop is the following:</p>
<p style="text-align: center;">
<strong>Programs can do this EXTREMELY fast</strong>
</p>
</section>
<section id="implementation" class="level2">
<h2 class="anchored" data-anchor-id="implementation">Implementation</h2>
<section id="solution---first-problem" class="level3">
<h3 class="anchored" data-anchor-id="solution---first-problem">Solution - First Problem</h3>
<p>My solution to the first coding problem:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.typing <span class="im">import</span> NDArray</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Helpful functions:</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co"># https://numpy.org/doc/stable/reference/generated/numpy.matmul.html</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># https://numpy.org/doc/stable/reference/generated/numpy.mean.html</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co"># https://numpy.org/doc/stable/reference/generated/numpy.square.html</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Solution:</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_model_prediction(<span class="va">self</span>, X: NDArray[np.float64], weights: NDArray[np.float64]) <span class="op">-&gt;</span> NDArray[np.float64]:</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        pred <span class="op">=</span> np.matmul(X, weights)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.<span class="bu">round</span>(pred, <span class="dv">5</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># X is an Nx3 NumPy array</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># weights is a 3x1 NumPy array</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># HINT: np.matmul() will be useful</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return np.round(your_answer, 5)</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_error(<span class="va">self</span>, model_prediction: NDArray[np.float64], ground_truth: NDArray[np.float64]) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        error <span class="op">=</span> np.mean(np.square(model_prediction <span class="op">-</span> ground_truth))</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">round</span>(error, <span class="dv">5</span>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># model_prediction is an Nx1 NumPy array</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>        <span class="co"># ground_truth is an Nx1 NumPy array</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># HINT: np.mean(), np.square() will be useful</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return round(your_answer, 5)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="solution---second-problem" class="level3">
<h3 class="anchored" data-anchor-id="solution---second-problem">Solution - Second Problem</h3>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.typing <span class="im">import</span> NDArray</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Solution:</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_derivative(<span class="va">self</span>, model_prediction: NDArray[np.float64], ground_truth: NDArray[np.float64], N: <span class="bu">int</span>, X: NDArray[np.float64], desired_weight: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># note that N is just len(X)</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">-</span><span class="dv">2</span> <span class="op">*</span> np.dot(ground_truth <span class="op">-</span> model_prediction, X[:, desired_weight]) <span class="op">/</span> N</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_model_prediction(<span class="va">self</span>, X: NDArray[np.float64], weights: NDArray[np.float64]) <span class="op">-&gt;</span> NDArray[np.float64]:</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.squeeze(np.matmul(X, weights))</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    learning_rate <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> train_model(</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, </span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        X: NDArray[np.float64], </span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        Y: NDArray[np.float64], </span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        num_iterations: <span class="bu">int</span>, </span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        initial_weights: NDArray[np.float64]</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> NDArray[np.float64]:</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_iterations):</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>            model_pred <span class="op">=</span> <span class="va">self</span>.get_model_prediction(X, initial_weights)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>            d1 <span class="op">=</span> <span class="va">self</span>.get_derivative(model_pred, Y, <span class="bu">len</span>(X), X, <span class="dv">0</span>)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>            d2 <span class="op">=</span> <span class="va">self</span>.get_derivative(model_pred, Y, <span class="bu">len</span>(X), X, <span class="dv">1</span>)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>            d3 <span class="op">=</span> <span class="va">self</span>.get_derivative(model_pred, Y, <span class="bu">len</span>(X), X, <span class="dv">2</span>)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>            initial_weights[<span class="dv">0</span>] <span class="op">=</span> initial_weights[<span class="dv">0</span>] <span class="op">-</span> d1 <span class="op">*</span> <span class="va">self</span>.learning_rate</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>            initial_weights[<span class="dv">1</span>] <span class="op">=</span> initial_weights[<span class="dv">1</span>] <span class="op">-</span> d2 <span class="op">*</span> <span class="va">self</span>.learning_rate</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>            initial_weights[<span class="dv">2</span>] <span class="op">=</span> initial_weights[<span class="dv">2</span>] <span class="op">-</span> d3 <span class="op">*</span> <span class="va">self</span>.learning_rate</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.<span class="bu">round</span>(initial_weights, <span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>I initially struggled with this problem due to some of the syntax and eventually had to look at the solution because I was so stuck. I was quite close in my solution (basing the derivative section off of what I did for multivariate gradient descent), however I originally thought that <code>desired_weight</code> was what we wanted the weights to be at the end of training, which was confusing. It’s really just pointing to which weight we want to update (so for <span class="math inline">\(w_1\)</span>, <code>desired_weight=0</code>. for <span class="math inline">\(w_2\)</span>, <code>desired_weight=1</code>, etc.).</p>
</section>
</section>
</section>
<section id="neural-networks-in-10-minutes---end-to-end-explanation" class="level1">
<h1>Neural Networks in 10 Minutes - End to End Explanation</h1>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/KL2EaTs8r8I" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/network.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Neural network architecture</figcaption>
</figure>
</div>
<p>Input attributes:</p>
<p><span class="math display">\[
\begin{align*}
x_1 &amp;\colon \text{parents' heights}\\
x_2 &amp;\colon \text{current height}\\
x_3 &amp;\colon \text{current weight}\\
\end{align*}
\]</span></p>
<section id="linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression">Linear Regression</h2>
<p>Need linear regression to explain the connections and the hidden layer. Here’s the formula again:</p>
<p><span class="math display">\[
y = w_1 x_1 + w_2 x_2 + w_3 x_3 + b
\]</span> Remember that <span class="math inline">\(x_n\)</span> are the input attributes and <span class="math inline">\(y\)</span> is the model’s output/prediction. The weights <span class="math inline">\(w_n\)</span> then determine the relative importance of each input parameter they are associated with. For example, <span class="math inline">\(w_1\)</span> determines how important <span class="math inline">\(x_1\)</span> (in this case the average parents’ height) is on someone’s final height (<span class="math inline">\(y\)</span>).</p>
<p>What about the bias?</p>
<p>In this context, it would probably be some base height, since a final height of zero doesn’t make much sense.</p>
</section>
<section id="hidden-layer" class="level2">
<h2 class="anchored" data-anchor-id="hidden-layer">Hidden Layer</h2>
<p>Each of the hidden nodes is doing linear regression. Each node is calculating some <span class="math inline">\(y\)</span> (<span class="math inline">\(y_1\)</span>, <span class="math inline">\(y_2\)</span>, <span class="math inline">\(y_3\)</span>, …) based on <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span>, and <span class="math inline">\(x_3\)</span> from the previous layer. This means that each node in the hidden layer (since it is calculating a <span class="math inline">\(y\)</span>) is learning a <span class="math inline">\(w_1\)</span>, <span class="math inline">\(w_2\)</span>, and <span class="math inline">\(w_3\)</span> independently of the other nodes in that same layer (i.e.&nbsp;<span class="math inline">\(y_1 \neq y_2\)</span>).</p>
</section>
<section id="output-layer" class="level2">
<h2 class="anchored" data-anchor-id="output-layer">Output Layer</h2>
<p>We can think of the two outputs as <span class="math inline">\(O_1\)</span> and <span class="math inline">\(O_2\)</span>. Just as in the previous layer, <span class="math inline">\(O_1\)</span> and <span class="math inline">\(O_2\)</span> are also calculated using a linear regression formula:</p>
<p><span class="math display">\[
\begin{align*}
O_1 &amp;= w_1 y_1 + w_2 y_2 + w_3 y_3 + w_4 y_4 + b\\
O_2 &amp;= w_1 y_1 + w_2 y_2 + w_3 y_3 + w_4 y_4 + b\\
\end{align*}
\]</span></p>
<p>BUT the difference here is that the <span class="math inline">\(w\)</span>’s and the <span class="math inline">\(b\)</span> for <span class="math inline">\(O_1\)</span> are learned independently from <span class="math inline">\(O_2\)</span> and are going to be different.</p>
<p>If the point of the model is to predict someone’s final height, that final prediction may be from averaging <span class="math inline">\(O_1\)</span> and <span class="math inline">\(O_2\)</span>. Training then is figuring out what <span class="math inline">\(w_1\)</span>, <span class="math inline">\(w_2\)</span>, <span class="math inline">\(w_3\)</span>, <span class="math inline">\(w_4\)</span>, and <span class="math inline">\(b\)</span> should be for <span class="math inline">\(y_1\)</span> - <span class="math inline">\(y_4\)</span> such that the error is minimized. The same thing is done for <span class="math inline">\(O_1\)</span> and <span class="math inline">\(O_2\)</span>.</p>
</section>
</section>
<section id="intro-to-pytorch.-forget-tensorflow" class="level1">
<h1>Intro to PyTorch. Forget TensorFlow</h1>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/SxwtCEDHXe8" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>PyTorch might be the only library that is needed. Might need some other libraries like NumPy or Pandas, but can usually get away with only using PyTorch. Fundamental data type here is the <strong>Tensor</strong>. PyTorch (and the Tensor data type more specifically) will also take care of all the ugly math, like nasty derivatives and huge matrix multiplications.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> a <span class="op">=</span> torch.ones(<span class="dv">5</span>, <span class="dv">5</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(a)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>tensor([[<span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>],</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>],</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>],</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>],</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>First two important functions in PyTorch is <code>sum()</code> and <code>mean()</code>:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="bu">sum</span> <span class="op">=</span> torch.<span class="bu">sum</span>(a, axis<span class="op">=</span><span class="dv">1</span>) <span class="co"># Note that axis=1 corresponds to the rows, NOT the columns</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(<span class="bu">sum</span>) <span class="co"># Print the sum of each column</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>tensor([<span class="fl">5.</span>, <span class="fl">5.</span>, <span class="fl">5.</span>, <span class="fl">5.</span>, <span class="fl">5.</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Also have <code>squeeze()</code> and <code>unsqueeze()</code>, which is used when we have unnecessary dimensions</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> a <span class="op">=</span> torch.ones(<span class="dv">5</span>, <span class="dv">1</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(a.shape)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>torch.Size([<span class="dv">5</span>, <span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>But the 1 is kind of unnecessary. Saying 5 x 1 is kind of unnecessary. So if we want to get rid of the 1 we can use the <code>squeeze()</code> method:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(a.shape)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> squeezed <span class="op">=</span> torch.squeeze(a)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(squeezed.shape)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>torch.Size([<span class="dv">5</span>, <span class="dv">1</span>])</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>torch.Size([<span class="dv">5</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>While this may seem like a small difference, it will have an impact on functions later on. Here’s the difference in a more visual way:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(a)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(squeezed)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>tensor([[<span class="fl">1.</span>],</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">1.</span>],</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">1.</span>],</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">1.</span>],</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">1.</span>]])</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>tensor([<span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If we can squeeze something, we should also have some way to unsqueeze as well. If we’re passing in two tensors, like one for training and one for validation, they both need to be the same size. We can’t have one look like <code>a</code> and one look like <code>squeezed</code>.</p>
<p>Here, the argument <code>dim</code> is where we want to insert that extra dimension. So to make our tensor <code>unsqueezed</code> 5x1 instead of just 5, we pass <code>dim=1</code>:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> unsqueezed <span class="op">=</span> torch.unsqueeze(squeezed, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(unsqueezed.shape)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(unsqueezed)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>torch.Size([<span class="dv">5</span>, <span class="dv">1</span>])</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>tensor([[<span class="fl">1.</span>],</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">1.</span>],</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">1.</span>],</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">1.</span>],</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        [<span class="fl">1.</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>If instead we pass <code>dim=0</code>, we get the opposite:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> unsqueezed <span class="op">=</span> torch.unsqueeze(squeezed, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(unsqueezed.shape)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(unsqueezed)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>torch.Size([<span class="dv">1</span>, <span class="dv">5</span>])</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>tensor([[<span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>, <span class="fl">1.</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="defining-neural-network-models-in-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="defining-neural-network-models-in-pytorch">Defining Neural Network Models in PyTorch</h2>
<p>We want something that looks like this:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MyModel:</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Constructor - layers</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Forward pass - get_model_prediction(example_datapoint)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="module-class" class="level3">
<h3 class="anchored" data-anchor-id="module-class"><code>Module</code> Class</h3>
<p>PyTorch has something like this, it’s called the <a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html"><code>Module</code></a> class. A <code>Module</code> is basically the same thing as a model. <strong>Every model we create in PyTorch is going to inherit or subclass <code>torch.nn.Module</code></strong>.</p>
</section>
<section id="linear-class" class="level3">
<h3 class="anchored" data-anchor-id="linear-class"><code>Linear</code> Class</h3>
<p>Remember that each layer in a neural network contains a bunch of nodes that are just doing linear regression based on the previous layer’s input attributes, so we need some class that can take in the previous layer’s input attributes, do linear regression, and then output them. That is what the <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"><code>Linear</code></a> class does.</p>
</section>
<section id="example" class="level3">
<h3 class="anchored" data-anchor-id="example">Example</h3>
<p>If you just Google “Neural network”, you might find something like this:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/neural_network.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Neural network architecture. <a href="https://victorzhou.com/series/neural-networks-from-scratch/">Source</a></figcaption>
</figure>
</div>
<p>Let’s build it!</p>
<p>Looks like our input layer has four nodes, and we have two hidden layers and one output layer. So we’re going to have three instances of <code>nn.Linear</code>.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MyModel(nn.Module):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create constructor</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_layer <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">4</span>, out_features<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.second_layer <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">6</span>, out_features<span class="op">=</span><span class="dv">6</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.final_layer <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">6</span>, out_features<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create forward pass</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Calling the forward method from the nn.Module class</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">#  first_layer_output = self.first_layer.forward(x)</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>       <span class="co"># Can also do the following:</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>       <span class="co">#  first_layer_output = self.first_layer(x)</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.final_layer(<span class="va">self</span>.second_layer(<span class="va">self</span>.first_layer.forward(x)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> model <span class="op">=</span> MyModel()</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> example_datapoint <span class="op">=</span> torch.randn(<span class="dv">1</span>, <span class="dv">4</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> model.forward(x<span class="op">=</span>example_datapoint) <span class="co"># Can also write model(x=example_datapoint)</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>tensor([[<span class="op">-</span><span class="fl">0.4633</span>, <span class="op">-</span><span class="fl">0.0682</span>]], grad_fn<span class="op">=&lt;</span>AddmmBackward0<span class="op">&gt;</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Then we need to train the model for some number of iterations. We can then actually use the model to get predictions.</p>
</section>
</section>
<section id="coding-problem-solution" class="level2">
<h2 class="anchored" data-anchor-id="coding-problem-solution">Coding Problem Solution</h2>
<div class="sourceCode" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtyping <span class="im">import</span> TensorType</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Helpful functions:</span></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># https://pytorch.org/docs/stable/generated/torch.reshape.html</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="co"># https://pytorch.org/docs/stable/generated/torch.mean.html</span></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co"># https://pytorch.org/docs/stable/generated/torch.cat.html</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="co"># https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Round your answers to 4 decimal places using torch.round(input_tensor, decimals = 4)</span></span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Solution:</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reshape(<span class="va">self</span>, to_reshape: TensorType[<span class="bu">float</span>]) <span class="op">-&gt;</span> TensorType[<span class="bu">float</span>]:</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.reshape(to_reshape, (to_reshape.size(dim<span class="op">=</span><span class="dv">1</span>)<span class="op">*</span>to_reshape.size(dim<span class="op">=</span><span class="dv">0</span>)<span class="op">//</span><span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> average(<span class="va">self</span>, to_avg: TensorType[<span class="bu">float</span>]) <span class="op">-&gt;</span> TensorType[<span class="bu">float</span>]:</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.mean(to_avg, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> concatenate(<span class="va">self</span>, cat_one: TensorType[<span class="bu">float</span>], cat_two: TensorType[<span class="bu">float</span>]) <span class="op">-&gt;</span> TensorType[<span class="bu">float</span>]:</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.cat((cat_one, cat_two), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_loss(<span class="va">self</span>, prediction: TensorType[<span class="bu">float</span>], target: TensorType[<span class="bu">float</span>]) <span class="op">-&gt;</span> TensorType[<span class="bu">float</span>]:</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.nn.functional.mse_loss(<span class="bu">input</span><span class="op">=</span>prediction, target<span class="op">=</span>target)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="dropout" class="level1">
<h1>Dropout</h1>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/VAoYgMo4FcA" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Dropout makes neural networks dumber on purpose. We do this to prevent overrfitting and to make a model generalize better.</p>
<blockquote class="blockquote">
<p><strong>Overfitting</strong>: When training accuracy is greater than testing accuracy.</p>
</blockquote>
<p>Said another way,: The model looks incredible when it is training and looking at training data, but the second you give it testing (i.e.&nbsp;real-world or non-training) data, it performs horribly. The model is essentially memorizing the training dataset and not really learning anything. Visually (and from a mathematical perspective), overfitting looks like this:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/fitting.png" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Overfitting of data. <a href="https://www.buildalpha.com/how-to-use-out-of-sample-data/">Source</a></figcaption>
</figure>
</div>
<p>Overfitting is usually caused by the model being too complex. Here, <em>complex</em> can mean a few things:</p>
<ul>
<li>The model has too many layers</li>
<li>Each layer has too many nodes</li>
</ul>
<p>Essentially, the model is just an equation, and right now that equation is way too complex. This is like having data that can be fit with just a linear equation, but a 10th order Taylor series is being used to approximate it.</p>
<p>Dropout aims to solve these problems. Let’s say we have a network like this:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/dropout.svg" class="img-fluid figure-img" style="width:60.0%"></p>
<figcaption>Dropout neural network architecture</figcaption>
</figure>
</div>
<p>Let’s say that we apply dropout to the entire output layer, but dropout only ends up applying to the first output node, <span class="math inline">\(O_1\)</span>. This means that this node is essentially turned off and its connections to the previous layer are severed (see the dotted lines). Mathematically, this equates to setting the equation for <span class="math inline">\(O_1\)</span> equal to zero:</p>
<p><span class="math display">\[
O_1 = w_1 x + w_2 y + w_3 z + b = 0
\]</span></p>
<p>In PyTorch, this would look like <code>nn.Dropout(p=0.2)</code>, where <code>p</code> is the propbability that the node is turned off. If we apply dropout to the entire output layer, each node would be turned off (set output or activation equal to zero) independently with probability <span class="math inline">\(p\)</span>.</p>
<p>Remember, overfitting occurs ebcause our model is too <em>complex</em>. So if dropout is to work, it has to <em>reduce</em> our model’s complexity. And we can see that is exactly what is happening. In other words, what dropout is doing is making the model more stupid so it doesn’t focus on unnecessary/irrelevant noise.</p>
<p>Dropout has been found to increase teseting accuracy, especially as models get deeper (i.e.&nbsp;have more layers).</p>
<section id="coding-problem-solution-1" class="level2">
<h2 class="anchored" data-anchor-id="coding-problem-solution-1">Coding Problem Solution</h2>
<p>This was a very poorly written question. It should have been stated as the following:</p>
<p>Model architecture takes in a 28*28 image and sends that to 512 neurons (use a linar layer for this). This is then followed by a ReLU activation function and then a dropout layer with probability <code>p=0.2</code>. Then there is a final linear layer that shrinks 512 neurons down to 10, followed finally by a sigmoid activation function.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtyping <span class="im">import</span> TensorType</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Solution(nn.Module):</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>        torch.manual_seed(<span class="dv">0</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">784</span>, out_features<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU()</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.drop <span class="op">=</span> nn.Dropout(p<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.second_linear <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">512</span>, out_features<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sigmoid <span class="op">=</span> nn.Sigmoid()</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define the architecture here</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, images: TensorType[<span class="bu">float</span>]) <span class="op">-&gt;</span> TensorType[<span class="bu">float</span>]:</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>        torch.manual_seed(<span class="dv">0</span>)</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span>  <span class="va">self</span>.sigmoid(<span class="va">self</span>.second_linear(<span class="va">self</span>.drop(<span class="va">self</span>.relu(<span class="va">self</span>.first_linear(images)))))</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Return the model's prediction to 4 decimal places</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.<span class="bu">round</span>(out, decimals<span class="op">=</span><span class="dv">4</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="training-neural-networks-in-pytorch" class="level1">
<h1>Training Neural Networks in PyTorch</h1>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/vVrASMLVBnY" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">0</span>)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DigitRecognition(nn.Module):</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        torch.manual_seed(<span class="dv">0</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.first_linear <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">784</span>, out_features<span class="op">=</span><span class="dv">512</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.relu <span class="op">=</span> nn.ReLU()</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.drop <span class="op">=</span> nn.Dropout(p<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.second_linear <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">512</span>, out_features<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sigmoid <span class="op">=</span> nn.Sigmoid()</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Define the architecture here</span></span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, images: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        torch.manual_seed(<span class="dv">0</span>)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span>  <span class="va">self</span>.sigmoid(<span class="va">self</span>.second_linear(<span class="va">self</span>.drop(<span class="va">self</span>.relu(<span class="va">self</span>.first_linear(images)))))</span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>How do we actually update this model over many iterations? Here we’re going to look at the training loop. The following code is the same regardless of the neural network:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DigitRecognition()</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>loss_function <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters())</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(epochs):</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> images, labels <span class="kw">in</span> train_dataloader():</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        images <span class="op">=</span> images.view(images.shape[<span class="dv">0</span>], <span class="dv">784</span>)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Training body</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        model_prediction <span class="op">=</span> model(images)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>        loss.loss_function(model_prediction, labels)</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Let’s break this down a bit:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DigitRecognition()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is instantiating our model. This could change depending on the task we are doing, but we still need to instantiate some kind of model.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>loss_function <span class="op">=</span> nn.CrossEntropyLoss()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is our loss function. Here it’s cross-entropy loss instead of MSE since we are doing a classification problem instead of a regression problem. Again, the loss function will vary depending on the application, but we still need one.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.Adam(model.parameters())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is an object in PyTorch that does gradient descent for us. What we need to pass in are the weights (parameters). <code>Adam</code> is gradient descent on steroids. It is still doing gradient descent (with a default learning rate), but is optimized to dynamically change the learning rate.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="dv">5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>An <em>epoch</em> is a single pass through the entire dataset. So here, the model is passing through the entire training dataset 5 times. Too many epochs might result in overfitting, too few might mean the weights haven’t been updated enough.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> images, labels <span class="kw">in</span> train_dataloader():</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><code>train_dataloader()</code> is just a sequence of steps to take in the raw data and do any preprocessing on it before it is fed to the network (for images, this might be things like resizing, cropping, or random reflections).</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>images <span class="op">=</span> images.view(images.shape[<span class="dv">0</span>], <span class="dv">784</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Could have also used <code>images = images.reshape(...)</code>, but we’re reshaping the image from 28 * 28 to a single vector with length 784 so that the network (which has a first layer with 784 neurons) can take it in.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>model_prediction <span class="op">=</span> model(images)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is calling the <code>forward()</code> method of the <code>model</code> class we defined earlier and gets our model predictions.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>optimizer.zero_grad()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is a frustrating line that really shouldn’t be required in PyTorch, but it cancels out all of the derivatives that were calculated from the previous iteration of gradient descent. This is done because for each iteration, we want to calculate the derivatives again so that we can update our weights. This line resets the derivatives so we can do this.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>loss.loss_function(model_prediction, labels)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is calculating the loss/error based on our current model’s prediction and the labels.</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>loss.backward()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Potentially the most important line here. This will calculate every single derivative necessary to perform gradient descent. This is the most computationally intensive step of this entire program. It is calculating all of the derivatives and is storing them in such a way that we can use them later.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>optimizer.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This updates all of the weights, so essentially doing <span class="math inline">\(\text{new weight} = \text{old weight} - \text{derivative} \cdot \text{learning rate}\)</span>. Next chunk:</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> images, labels <span class="kw">in</span> test_dataloader:</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    images <span class="op">=</span> images.view(images.shape[<span class="dv">0</span>], <span class="dv">784</span>)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    model_prediction <span class="op">=</span> model(images)</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">max</span>, idx <span class="op">=</span> torch.<span class="bu">max</span>(model_prediction, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(images)):</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>        plt.imshow(images[i].view(<span class="dv">28</span>, <span class="dv">28</span>))</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        plt.show()</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(idx[i].item())</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This is showing us how well the model is doing once training is done.</p>
<div class="sourceCode" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This puts the model in evaluation mode. We’re telling PyTorch that we just want the model’s predictions, so it doesn’t need to calculate any derivatives for training.</p>
<p>Then we iterate over our test dataloader, reshape our images, pass them into the model, and get our predictions. For every image, we’re predicting a bunch of probabilities (specifically 10). So we should take the maximum probability and map that to the class. This lets us see the image and the corresponding digit the model thinks it is. Then iterate over each image and do this.</p>
</section>
<section id="intro-to-nlp" class="level1">
<h1>Intro to NLP</h1>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/mjeXXssPLOE" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Solving a problem with no background in NLP that requires an understanding of NLP to complete is stupid. So I’m watching the video first.</p>
<p><strong>Tokenization</strong>: How text is broken up. Is it broken up into words, characters, parts of words, etc. * How will each token be <strong>encoded</strong>, or converted from a raw text string to an integer?</p>
<p>what we need to do ehre is map each token to an integer. So we need some dictionary that maps tokens to integers.</p>
<section id="the-setup" class="level2">
<h2 class="anchored" data-anchor-id="the-setup">The Setup</h2>
<p>First need to construct a list of all the unique words in the dataset. Something that can do this is a <strong>hash set</strong>, or a <strong>set</strong> in Python. We can then convert the set to a list, sort it, and create the dictionary.</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Solution:</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_dataset(<span class="va">self</span>, positive: List[<span class="bu">str</span>], negative: List[<span class="bu">str</span>]) <span class="op">-&gt;</span> TensorType[<span class="bu">float</span>]:</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>        vocabulary <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> sentence <span class="kw">in</span> positive: <span class="co"># Loop through each sentence in the positive dataset</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> word <span class="kw">in</span> sentence.split(): <span class="co"># Extract each word in the sentence</span></span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>                vocabulary.add(word) <span class="co"># Use .add() instead of .append() since .add() is for sets, .append() is for lists</span></span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> sentence <span class="kw">in</span> negative:</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> word <span class="kw">in</span> setence.split():</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>                vocabulary.add(word)</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>        list_convert <span class="op">=</span> <span class="bu">list</span>(vocabulary) <span class="co"># Convert vocabulary to a list</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>        sorted_list <span class="op">=</span> <span class="bu">sorted</span>(list_convert) <span class="co"># Get sorted list from low to high like we talked about earlier</span></span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Now we want to build our dictionary to convert words to integers</span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>        word_to_int <span class="op">=</span> {}</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(sorted_list)):</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>            word_to_int[sorted_list[i]] <span class="op">=</span> i <span class="op">+</span> <span class="dv">1</span> <span class="co"># Key is the index in the list, value is starting at the smallest and going up from there (i+1 since we have zero indexing)</span></span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Encode every sentence as a tensor of integers</span></span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>        tensors <span class="op">=</span> []</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> sentence <span class="kw">in</span> positive:</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>            current_list <span class="op">=</span> []</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> word <span class="kw">in</span> sentence.split():        </span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>                current_list.append(word_to_int[word]) <span class="co"># Getting the integer conversion for that word and putting it into a list</span></span>
<span id="cb34-29"><a href="#cb34-29" aria-hidden="true" tabindex="-1"></a>            tensors.append(torch.tensor(current_list)) <span class="co"># Convert current_list to a tensor and append it to tensors</span></span>
<span id="cb34-30"><a href="#cb34-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb34-31"><a href="#cb34-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> sentence <span class="kw">in</span> negative:</span>
<span id="cb34-32"><a href="#cb34-32" aria-hidden="true" tabindex="-1"></a>            current_list <span class="op">=</span> []</span>
<span id="cb34-33"><a href="#cb34-33" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> word <span class="kw">in</span> sentence.split():        </span>
<span id="cb34-34"><a href="#cb34-34" aria-hidden="true" tabindex="-1"></a>                current_list.append(word_to_int[word]) <span class="co"># Getting the integer conversion for that word and putting it into a list</span></span>
<span id="cb34-35"><a href="#cb34-35" aria-hidden="true" tabindex="-1"></a>            tensors.append(torch.tensor(current_list)) <span class="co"># Convert current_list to a tensor and append it to tensors</span></span>
<span id="cb34-36"><a href="#cb34-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-37"><a href="#cb34-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> nn.utils.rnn.pad_sequence(tensors, batch_first<span class="op">=</span><span class="va">True</span>) <span class="co"># Pad tensors with zeros so they are all the same length</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="machine-learning-on-tweets" class="level1">
<h1>Machine Learning on Tweets</h1>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/8dRLaVOlTYA" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p><strong>Sentiment analysis</strong>: Given some text (sentence, paragraph, etc.), we want to feed this into some kind of model:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    A(Text) --&gt; B(Neural network)
    B --&gt; C{Evaluate}
    C  --&gt; D(Positive)
    C --&gt; E(Negative)
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>There are many applications for this, like Tweets impacting the stock market. One important concept here is that of <strong>embeddings</strong>:</p>
<p><strong>Embeddings</strong>: Learning a vectorized representation of every token in our model’s vocabulary.</p>
<p>Example sentence: “I loved that movie” might be mapped in the following way:</p>
<p><span class="math display">\[
\begin{align*}
\text{I} &amp;\rightarrow 0\\
\text{loved} &amp;\rightarrow 2\\
\text{that} &amp;\rightarrow 1\\
\text{movie} &amp;\rightarrow 4
\end{align*}
\]</span> So we would feed the following vector into the model: [0, 2, 1, 4]. The first step is the model understanding the meaning of these tokens independently. Remember, the specific embeddings here are arbitrary; the important thing is the <em>relationship</em> between the embeddings. These embedding vectors are tweaked during training.</p>
<p><strong>Embedding dimension</strong>: How long each embedding is (i.e.&nbsp;is a single token represented as [2], [2, 1, 4], [2, 1, 4, 6, 8], etc.) The higher this number, the more complex relationships our model can pick up on.</p>
<p>Embeddings start off randomly, but are then closer to other, similar embeddings. Mathematically, embeddings that are more similar to each other have a dot product that is large. For example, the dot product between the embeddings for “man” and “woman” would be higher than the dot product between the embeddings for “leaf” and “chair”.</p>
<section id="model-architecture" class="level2">
<h2 class="anchored" data-anchor-id="model-architecture">Model Architecture</h2>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    A(Input, size BxT) --&gt; B(Embedding layer)
    B -- BxTxC --&gt; C(Avg.)
    C -- BxC --&gt; D(Linear layer)
    D -- Bx1 --&gt; E(Sigmoid)
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>With the final output being a number between zero and one for every input example in our training batch. Example breaking down the variables:</p>
<p><span class="math display">\[
\begin{bmatrix}
\text{I} &amp; \text{loved} &amp; \text{that} &amp; \text{movie}\\
\text{I} &amp; \text{hated} &amp; \text{that} &amp; \text{movie}\\
\end{bmatrix}
\]</span> here, <span class="math inline">\(B=2\)</span> and <span class="math inline">\(T = 4\)</span>, with <span class="math inline">\(C\)</span> being our embedding dimension.</p>
<section id="lookup-table" class="level3">
<h3 class="anchored" data-anchor-id="lookup-table">Lookup Table</h3>
<p>Let’s say that the following mappings are true:</p>
<p><span class="math display">\[
\begin{align*}
\text{I} &amp;\rightarrow 0 \rightarrow[0, 0, 1, 0, 0, 0]\\
\text{loved} &amp;\rightarrow 2 \rightarrow[1, 0, 0, 0, 0, 0]\\
\text{that} &amp;\rightarrow 1 \rightarrow[0, 0, 0, 1, 0, 0]\\
\text{movie} &amp;\rightarrow 4 \rightarrow[0, 0, 0, 0, 1, 0]
\end{align*}
\]</span> We could create a new matrix where each how is the learned/trained representations of the model’s covabulary in each row. If we matrix multiply these two matrices, we get the learned representations of our model’s vocabulary. This is an example of <strong>one-hot encoding</strong> (or one-hot input).</p>
</section>
</section>
<section id="coding-problem-solution-2" class="level2">
<h2 class="anchored" data-anchor-id="coding-problem-solution-2">Coding Problem Solution</h2>
<div class="sourceCode" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtyping <span class="im">import</span> TensorType</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Solution(nn.Module):</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocabulary_size: <span class="bu">int</span>):</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>        torch.manual_seed(<span class="dv">0</span>)</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.embedding_layer <span class="op">=</span> nn.Embedding(vocabulary_size, <span class="dv">16</span>)</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear_1 <span class="op">=</span> nn.Linear(in_features<span class="op">=</span><span class="dv">16</span>, out_features<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sigmoid <span class="op">=</span> nn.Sigmoid()</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: TensorType[<span class="bu">int</span>]) <span class="op">-&gt;</span> TensorType[<span class="bu">float</span>]:</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Hint: The embedding layer outputs a B, T, embed_dim tensor</span></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># but you should average it into a B, embed_dim tensor before using the Linear layer</span></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>        embeddings <span class="op">=</span> <span class="va">self</span>.embedding_layer(x)</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>        averages <span class="op">=</span> torch.mean(embeddings, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>        linear <span class="op">=</span> <span class="va">self</span>.linear_1(averages)</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>        sigmoid_out <span class="op">=</span> <span class="va">self</span>.sigmoid(linear)</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.<span class="bu">round</span>(sigmoid_out, decimals<span class="op">=</span><span class="dv">4</span>)    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="illustrated-guide-to-attention---how-chatgpt-reads" class="level1">
<h1>Illustrated Guide to Attention - How ChatGPT Reads</h1>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/sjrvs7dJOvU" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Let’s start out by thinking of these large language models (LLMs)/generative pretrainted transformers (GPTs) as a black box:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    A(Words) --&gt; B[LLM/GPT]
    B --&gt; C(Response)
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>To get computers to read, it makes sense to consider how humans read. We don’t read word-by-word, but instead word-by-word while considering the relationships that came before the current one in a sentence. For example:</p>
<blockquote class="blockquote">
<p>Gertrude lead the horse The President has to lead Lead shields against X-rays Here, “lead” is the same work but has different meanings. For each input string of length <span class="math inline">\(T\)</span>, the model generates a score of how related two words are with each other.</p>
</blockquote>
<section id="the-mathematical-explanation" class="level2">
<h2 class="anchored" data-anchor-id="the-mathematical-explanation">The Mathematical Explanation</h2>
<p>Main idea: We want our model to be <strong>context-aware</strong> of the words in a sentence. Not just figure out the next word, but instead know how they’re related. Each word has two vectors associated with it: a <strong>key</strong> and a <strong>query</strong>. A query vector represents what the word is searching for (querying), and a word’s key vector might be what it has to offer the query.</p>
<p>Here’s an example sentence:</p>
<blockquote class="blockquote">
<p>The tall man runs.</p>
</blockquote>
<p>The query vector associated with “tall” might say that it is looking for a noun to describe, and the key vector associated with “man” might give an indication that it is a noun. We then take the dot product between every (for example) noun’s query and every (for example) adjective’s key. Those values populate something called the **attention matrix*, which is a measure of how related any two words in a sentence are to each other.</p>
<p>The key and query vectors for every word are calculated via linear regression. Before each word is passed into the attention block, each word is converted into an <strong>embedding vector</strong>. So we have one network that takes in embeddings and outputs queries, and another that takes in embeddings and outputs keys. We also have a <strong>value vector</strong>, which is then multiplied with the attention matrix, giving us the final output of the attention layer: “The <span class="math inline">\(\text{i}^{\text{th}}\)</span> entry in the new representation of a word is just the average of all the <span class="math inline">\(\text{i}^{\text{th}}\)</span> entries of the value vectors for the neighboring words weighted by the attention score for each pair of words, which factors in how important that pair is. Put shortly, the output of the attention matrix is an updated vector for every word that factors in the importance of the neighboring words.</p>


</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{gregory2024,
  author = {Gregory, Josh},
  title = {GPT and {Chill} {Notes}},
  date = {2024-06-07},
  url = {https://joshgregory42.github.io/posts/2024-06-07-gpt-chill/},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-gregory2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Gregory, Josh. 2024. <span>“GPT and Chill Notes.”</span> June 7, 2024.
<a href="https://joshgregory42.github.io/posts/2024-06-07-gpt-chill/">https://joshgregory42.github.io/posts/2024-06-07-gpt-chill/</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/www\.joshgregory42\.github\.io");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/joshgregory42/joshgregory42.github.io">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>