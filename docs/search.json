[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Josh Gregory",
    "section": "",
    "text": "Hi there!\nI am currently a graduate student at the University of Colorado Boulder pursuing a Master’s of Science in Mechanical Engineering. My thesis is focusing on using vision transformers to more effectively extract cerebrovasculature that could lead to stroke. I have been a member of FLOWLab at CU Boulder for a few years, focusing on blood clots, stroke mechanics, and developing better computational tools to understand them.\nI am passionate about helping people and creating tools and technologies to improve the lives of those around me. Take a look around to see what I’ve been up to!\n\n\nEducation\nMaster of Science, Mechanical Engineering, University of Colorado Boulder (2025)\nBachelor of Science, Mechanical Engineering, minors in Computer Science and Biomedical Engineering, University of Colorado Boulder (2024)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Current work\nI am currently a graduate student at University of Colorado Boulder studying Mechanical Engineering with a focus on artificial intelligence applied to biomedical imaging. I am a member of CU Boulder’s Biofluidics Laboratory (FlowLab), where I am researching the computational fluid dynamics (CFD) of blood clots. The goal of this research is to model the blood clotting process much more accurately than previously done, to allow medical professionals to make better clinical decisions.\n\n\nPast work\nI have worked at corporations such as Collins Aerospace and the National Renewable Energy Laboratory (NREL). At Collins Aerospace, I aided senior engineers in testing prototype optical devices for future usage on both commercial and military aircraft. I used my knowledge of computer-aided design (CAD) and 3D printing to design and manufacture testing apparatuses that enabled rapid prototyping and characterization of the prototype optical devices. At NREL my work focused on additively-manufacturing microencapsulated phase-change materials (MEPCMs) for use in thermal batteries and heat exchangers.\nAfter transferring to CU Boulder, I joined the liquid engine sub-team of CU’s Sounding Rocket Lab (SRL), where I ran the computational fluid dynamics (CFD) team simulating the nozzle and injector assemblies, specifically focusing on the mathematical models used to run the simulations.\nAt my previous institution, I was a member of Oregon State’s High Altitude Liquid Engine (HALE) team, where I ran multiphase CFD simulations on both the injector and nozzle assemblies using Oregon State’s supercomuter cluster to perform more detailed analyses.\nI was also a member of the Oregon State College of Psychology’s CARVE Laboratory, where I worked on the software necessary to better understand the interaction between pedestrians and autonomous vehicles.\nDuring high school, I wrote a paper where I derived a fundamental equation relating to rocket nozzle theory, which I then verified experimentally via computer simulations. I also worked on a new sounding rocket launch system, which will allow for a reduction in cost of up to 90% compared to regular sounding rocket launch systems."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "GPT and Chill Notes\n\n\n\n\n\n\nNotes\n\n\nDeep Learning\n\n\nMachine Learning\n\n\nArtificial Intelligence\n\n\n\nNotes from the GPT and Chill playlist\n\n\n\n\n\nJune 7, 2024\n\n\nJosh Gregory\n\n\n\n\n\n\n\n\n\n\n\n\nAdvice for Undergrads\n\n\n\n\n\n\nResearch\n\n\nAdvice\n\n\n\nReflections on my undergraduate mechanical engineering career since graduating\n\n\n\n\n\nJune 5, 2024\n\n\nJosh Gregory\n\n\n\n\n\n\n\n\n\n\n\n\nBackpropagation Calculus\n\n\n\n\n\n\nNotes\n\n\nDeep Learning\n\n\nMachine Learning\n\n\nArtificial Intelligence\n\n\n\nMy notes from the 3Blue1Brown video on the calculus behind backpropagation\n\n\n\n\n\nMay 6, 2024\n\n\nJosh Gregory\n\n\n\n\n\n\n\n\n\n\n\n\nMS Thesis Resources\n\n\n\n\n\n\nNotes\n\n\nDeep Learning\n\n\nMachine Learning\n\n\nArtificial Intelligence\n\n\nThesis\n\n\n\nResrouces for my MS Thesis\n\n\n\n\n\nMay 6, 2024\n\n\nJosh Gregory\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Excellence Award Statement\n\n\n\n\n\n\nResearch\n\n\nCollege\n\n\n\nReflections on my research\n\n\n\n\n\nMay 5, 2024\n\n\nJosh Gregory\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "",
    "section": "",
    "text": "Projects\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3mm Mini Laparoscopic Port\n\n\n\n\n\n\nsurgery\n\n\nmentoring\n\n\n\nCollaboration with Children’s Hospital Colorado to create a novel surgical port \n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Aesthetics in Design Portfolio\n\n\nPortfolio for Aesthetics in Design, Spring 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3mm Mini Laparoscopic Port\n\n\nCollaboration with Children’s Hospital Colorado to create a novel surgical port\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/test.html",
    "href": "projects/test.html",
    "title": "Amphia Sensium",
    "section": "",
    "text": "Stay tuned, more to follow soon.\nMeanwhile, read an interview with Frank Niemantsverdriet and myself on how Amphia uses AI to combat scarcity in healthcare (English, Dutch)."
  },
  {
    "objectID": "projects/senior_design.html",
    "href": "projects/senior_design.html",
    "title": "3mm Mini Laparoscopic Port",
    "section": "",
    "text": "Download a copy of the poster here"
  },
  {
    "objectID": "projects/senior_design.html#where",
    "href": "projects/senior_design.html#where",
    "title": "3mm Mini Laparoscopic Port",
    "section": "Where",
    "text": "Where"
  },
  {
    "objectID": "projects/senior_design.html#where-childrens-hospital-colorado-comes-in",
    "href": "projects/senior_design.html#where-childrens-hospital-colorado-comes-in",
    "title": "3mm Mini Laparoscopic Port",
    "section": "Where Children’s Hospital Colorado comes in",
    "text": "Where Children’s Hospital Colorado comes in"
  },
  {
    "objectID": "projects/senior_design.html#childrens-hospital-colorado-comes-in",
    "href": "projects/senior_design.html#childrens-hospital-colorado-comes-in",
    "title": "3mm Mini Laparoscopic Port",
    "section": "Children’s Hospital Colorado comes in",
    "text": "Children’s Hospital Colorado comes in"
  },
  {
    "objectID": "projects/senior_design.html#childrens-hospital-colorado",
    "href": "projects/senior_design.html#childrens-hospital-colorado",
    "title": "3mm Mini Laparoscopic Port",
    "section": "Children’s Hospital Colorado",
    "text": "Children’s Hospital Colorado"
  },
  {
    "objectID": "projects/senior_design.html#infants",
    "href": "projects/senior_design.html#infants",
    "title": "3mm Mini Laparoscopic Port",
    "section": "Infants",
    "text": "Infants\nThese ports work great for adults, when the thickness of the abdominal cavity is enough to keep the port in place and the patient is large enough that there is enough room to maneuver the laparoscopic tools without the surgical field getting too crowded. Both of these go completely out the window when working on"
  },
  {
    "objectID": "projects/senior_design.html#babies-in-the-nicu",
    "href": "projects/senior_design.html#babies-in-the-nicu",
    "title": "3mm Mini Laparoscopic Port",
    "section": "Babies in the NICU",
    "text": "Babies in the NICU\nThese ports work great for adults, when the thickness of the abdominal cavity is enough to keep the port in place and the patient is large enough that there is enough room to maneuver the laparoscopic tools without the surgical field getting too crowded. Both of these go completely out the window when working on babies in the NICU, which is where Colorado Children’s Hospital comes into the picture.\nOur client for this project was a pediatric surgeon from Colorado Children’s Hospital, and one of the first things he told us was the size of the surgical field. It’s the size of a grapefruit. You read that right. A grapefruit."
  },
  {
    "objectID": "projects/senior_design.html#babies-in-the-neonatal-icu",
    "href": "projects/senior_design.html#babies-in-the-neonatal-icu",
    "title": "3mm Mini Laparoscopic Port",
    "section": "Babies in the Neonatal ICU",
    "text": "Babies in the Neonatal ICU\nThese ports work great for adults, when the thickness of the abdominal cavity is enough to keep the port in place and the patient is large enough that there is enough room to maneuver the laparoscopic tools without the surgical field getting too crowded. Both of these go completely out the window when working on babies in the NICU, which is where Colorado Children’s Hospital comes into the picture.\nOur client for this project was a pediatric surgeon from Colorado Children’s Hospital, and one of the first things he told us was the size of the surgical field. It’s the size of a grapefruit. You read that right. A grapefruit."
  },
  {
    "objectID": "projects/senior_design.html#babies-in-the-neonatal-itensive-care-u",
    "href": "projects/senior_design.html#babies-in-the-neonatal-itensive-care-u",
    "title": "3mm Mini Laparoscopic Port",
    "section": "Babies in the Neonatal Itensive Care U",
    "text": "Babies in the Neonatal Itensive Care U\nThese ports work great for adults, when the thickness of the abdominal cavity is enough to keep the port in place and the patient is large enough that there is enough room to maneuver the laparoscopic tools without the surgical field getting too crowded. Both of these go completely out the window when working on babies in the NICU, which is where Colorado Children’s Hospital comes into the picture.\nOur client for this project was a pediatric surgeon from Colorado Children’s Hospital, and one of the first things he told us was the size of the surgical field. It’s the size of a grapefruit. You read that right. A grapefruit."
  },
  {
    "objectID": "projects/senior_design.html#babies-in-the-neonatal-itensive-care-unit-nicu",
    "href": "projects/senior_design.html#babies-in-the-neonatal-itensive-care-unit-nicu",
    "title": "3mm Mini Laparoscopic Port",
    "section": "Babies in the Neonatal Itensive Care Unit (NICU)",
    "text": "Babies in the Neonatal Itensive Care Unit (NICU)\nThese ports work great for adults, when the thickness of the abdominal cavity is enough to keep the port in place and the patient is large enough that there is enough room to maneuver the laparoscopic tools without the surgical field getting too crowded. Both of these go completely out the window when working on babies in the NICU, which is where Colorado Children’s Hospital comes into the picture.\nOur client for this project was a pediatric surgeon from Colorado Children’s Hospital, and one of the first things he told us was the size of the surgical field. It’s the size of a grapefruit. You read that right. A grapefruit."
  },
  {
    "objectID": "posts/2024-05-05-research-award/index.html",
    "href": "posts/2024-05-05-research-award/index.html",
    "title": "Research Excellence Award Statement",
    "section": "",
    "text": "Every year, CU Boulder’s College of Engineering & Applied Science hands out awards to certain graduating students. Faculty and students from the engineering deparments within the College (i.e. mechanical, aerospace, civil, etc.) nominates students, who then submit statements for their individual award. Some of them are then selected by the College as a whole. One of them is the Research Excellence Award, which I was fortunate enough to receive from the College after being nominated by a few faculty members. As part of the selection process, each nominee had to write a statement about research and any advice they would give to other students. What follows is my statement.\n\nResearch lets me act like a curious five-year-old. An unwavering sense of wonder and curiosity about the world is what led me to conducting research in various fields for each year of my undergraduate career. During my time at CU, I have contributed to research at the intersection of biology and mechanical engineering under Dr. Debanjan Mukherjee, worked with the National Renewable Energy Laboratory (NREL) on fundamental research in heat transfer and thermodynamics, and more recently began my Master’s thesis on the topic of artificial neural networks applied to brain vasculature to better predict and understand strokes. The commonality between all of these projects is an understanding and application of fundamentals across varying fields, a perspective that is helpful in both uniting different fields and forging new paths at their intersections.\nMy work with Dr. Mukherjee is currently investigating the mechanical properties of blood clots in a dynamic and transient sense. That is, using in vitro experiments to inform in silico models of blood clots. The novel fusion of these two typically disparate approaches allows us to extract the boundaries of blood clots as they develop over time, allowing for significantly more accurate computational modeling of blood clot dynamics. My contribution has been on the image processing side, assisting PhD student Chayut Teeraratkul in determining the techniques to extract the blood clot boundaries. My work enabled the group to obtain correct image processing parameters for a wide range of experiments, allowing for our code to be applied to more datasets. This work is currently being written for publication and is going to be published in the following months, with the code set for an open-source release to the community. It is inherently transdisciplinary, utilizing knowlege from mechanical engineering, biology, and computer science. This has allowed me to gain familiarity with diverse areas such as blood clot hemodynamics, image processing, and software development. Recognizing the importance of this work, CU awarded it a UROP grant in the summer of 2023.\nAlso during the summer of 2023, I interned at NREL, working on additively manufacturing heat exchangers for use in energy storage. Sometimes called a “thermal battery”, these materials utilize microscopic phase-change materials to store energy in the form of heat, in contrast to the status quo that is a chemical reaction. This work required knowledge of heat transfer, materials science, thermodynamics, and fluid mechanics, but also allowed me to work with scientists in other fields, such as chemistry. Throughout the summer, I was able to determine the optimal 3D printing settings for these heat exchangers and also gained experience in thermodynamic characterization, utilizing machines such as differential scanning calorimeters. This work has the potential to contribute significantly to energy storage issues of our time. Current lithium-ion batteries are not adequate for large scale storage of energy, such as that generated by solar panels. Furthermore, it allows the insulation of houses to store incoming sunlight, drastically reducing the cost of the monthly utility bill. Due to its manufacturing process, it also allows for cheaper cooling and heating solutions for lower-income communities in a world beset by climate change. My work was presented in a poster session within NREL and is currently in manuscript form to be published soon.\nMore recently, I have begun work on my Master’s Thesis, which focuses on one of the most crucial tasks in Dr. Mukherjee’s lab: automated segmentation of the M2, P2, and A2 vessels of the brain, which are increasingly found to be the locations of stroke. As such, understanding the fluid mechanics of these vessels is vital to predicting stroke risk. The goal of my thesis is to take CT images from clinicians and feed them to a neural network to extract these vessels, allowing for significantly better stroke risk assessment in the research and clinical settings. This is also a collaboration between our lab and others, namely Drs. Timothy Stalker and Maruzio Pacifici from Thomas Jefferson University, Dr. Vincent Tutino from the University of Buffalo, and other collaborators from UMC Amsterdam and CU Anschutz. This is a collaboration across fields as well as researchers, requiring knowledge not only of the mechanical engineering for which I am familiar, but also computer science, image processing, and biology.\nMy best advice to a future student is to embrace curiosity and approach challenges with the mindset of a five-year-old. Be curious, insatiably so. Research allows you to work on problems you never thought possible with people in incredible fields. It allows you to learn how to think through problems with others, collaborating to uncover or create something new in this world. On the other hand, resist the temptation to become overly specialized. Put yourself in a position to learn new things. Ignorance can be helpful when utilized in the correct way. The fundamentals of engineering are just that; the fundamentals. How you apply them is up to you. My career trajectory initially pointed towards the aerospace industry. As a freshman, I ran computational fluid dynamics simulations of rocket engines, eventually progressing to the simulation lead for various rocket teams. However, the pandemic sparked a desire to contribute to the medical field. Recognizing the shared fundamentals of fluid mechanics between rocket engines and the cardiovascular system, I pivoted, embracing my curiosity and desire to help others, despite my ignorance in the field.\nTwo other lessons that I found useful in both research and life: question assumptions and find the nuance. Assumptions in research are often the most challenging part of a project, so gaining experience in making them helps you become better in your field as well as in life. Finding nuance is also key. In today’s world it is easy to make blanket statements and assume that either you know everything, or the person you are working for does. Research has a way of humbling everyone, no matter their stature. Not only will you learn to find nuance in your own ideas, but finding nuance in the ideas of others leads to new avenues of exploration in ways no one anticipates. This non-linear path is truly one of the most rewarding areas of research. Yes, it is littered with disappointing days, failed experiments, and false hopes. But it is also marked by success, new ideas, perseverance, and a curiosity to better understand the world around us.\nParticipation in research is participation in the scientific process. The same scientific process that Isaac Newton, Galileo Galilei, Madame Curie, and Jennifer Doudna all were a part of. It is a uniquely human experience that ties you to a lineage of people who create discoveries and technologies that cause the world to fundamentally change. Act like a five-year-old and celebrate your ignorance. Not knowing how difficult something is might be the very reason you end up doing it and changing the world.\n\n\n\nCitationBibTeX citation:@online{gregory2024,\n  author = {Gregory, Josh},\n  title = {Research {Excellence} {Award} {Statement}},\n  date = {2024-05-05},\n  url = {https://joshgregory42.github.io/posts/2024-05-05-research-award/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGregory, Josh. 2024. “Research Excellence Award Statement.”\nMay 5, 2024. https://joshgregory42.github.io/posts/2024-05-05-research-award/."
  },
  {
<<<<<<< HEAD
    "objectID": "posts/2024-05-06-backprop-calc/index.html",
    "href": "posts/2024-05-06-backprop-calc/index.html",
    "title": "Backpropagation Calculus",
    "section": "",
    "text": "The video where these notes come from can be found here. The corresponding notes (which are probably better than mine) can be found here.\n\nLet’s say that we have a really simple neural network, just neurons connected to each other in series like this:\n\n\n\n\n\nflowchart LR\n    A( ) --&gt;|1| B( )\n    B --&gt; |2| C( )\n    C --&gt; |3| D( )\n\n\n\n\n\n\nWe want to see how sensitivbe the cost function is to \\(w_1\\), \\(w_2\\), \\(w_3\\) and \\(b_1\\), \\(b_2\\), \\(b_3\\). Knowledge of how the sensitivity of the weights and biases affects the cost function allows us to figure out how to minimize it. Here we’re just going to focus on the last two neurons. Call the weight of the current layer (labeled with a 3) \\(w^{(L)}\\), the activation of layer \\(L\\) being \\(a^{(L)}\\), and the activation of the previous layer \\(a^{(L-1)}\\), with the desired output \\(y\\) supplied by the training data. Assuming mean squared error (MSE), the cost function for this network for a single training example is\n\\[\nC_0 = \\left( a^{(L)} - y \\right)^2\n\\]\nBut we need to get \\(a^{(L)}\\) somehow. We can get that via\n\\[\na^{(L)} = \\sigma \\left( w^{(L)} a^{(L-1)} + b^{(L)} \\right)\n\\] where \\(\\sigma\\) is the activation function. Let’s call everything inside of the parantheses \\(z^{(L)}\\), therefore\n\\[\n\\begin{align*}\na^{(L)} &= \\sigma \\left( w^{(L)} a^{(L-1)} + b^{(L)} \\right)\\\\\n&= \\sigma \\left( z^{(L)} \\right)\n\\end{align*}\n\\]\nNotice what this means: if we give the network a single test data point \\(y\\), to calculate the cost function for that test point, we need to find \\(a^{(L)}\\), which in turn requires us to use all of the weights, biases, and activations of all the previous layers. We need to propagate backwards through our network.\nSo we have a few steps for each layer. We first need to get \\(z^{(L)}\\) with \\(w^{(L)}\\), \\(a^{(L-1)}\\), and \\(b^{(L)}\\). Then we go from \\(z^{(L)}\\) to \\(a^{(L)}\\). Then we use both \\(a^{(L)}\\) and \\(y\\) to calculate \\(C_0\\).\nObviously need to find \\(\\frac{ \\partial C_0 }{ \\partial w^{(L)} }\\) to minimize the cost function. Can use the chain rule for this:\n\\[\n\\frac{\\partial C_0}{ \\partial w^{(L)} } = \\frac{\\partial z^{(L)}}{ \\partial w^{(L)} } \\frac{\\partial a^{(L)}}{ \\partial z^{(L)} } \\frac{\\partial C_0}{ \\partial a^{(L)} }\n\\] here we’re going to again assume MSE, so\n\\[\nC_0 = \\left( a^{(L)} - y \\right)^2\n\\] however this found change depending on the cost function. Therefore,\n\\[\n\\begin{align*}\n\\frac{\\partial C_0}{ \\partial a^{(L)} } &= 2 \\left( a^{(L)} - y \\right)\\\\\n\\frac{\\partial a^{(L)}}{ \\partial z^{L} } &= \\sigma ' \\left( z^{(L)} \\right) = \\sigma ' \\left( w^{(L)} a^{(L-1)} + b^{(L)} \\right)\\\\\n\\frac{\\partial z^{(L)}}{ \\partial w^{(L)} } &= a^{(L-1)}\n\\end{align*}\n\\] Therefore,\n\\[\n\\frac{ \\partial C_0 }{ \\partial w^{(L)} } = a^{(L-1)} \\sigma ' \\left( z^{(L)} \\right) 2 \\left( a^{(L)} - y \\right)\n\\] For all training examples, we take the average:\n\\[\n\\frac{ \\partial C }{ \\partial w^{(L)} } = \\frac{1}{n} \\sum_{k=0}^{n-1} \\frac{ \\partial C_k }{ \\partial w^{(L)} }\n\\] Note that this is just one component in the \\(\\nabla C\\) vector, however:\n\\[\n\\nabla C =\n\\begin{bmatrix}\n\\frac{\\partial C}{\\partial w^{(1)}} \\\\\n\\frac{\\partial C}{\\partial b^{(1)}}\\\\\n\\vdots\\\\\n\\frac{\\partial C}{\\partial w^{(L)}}\\\\\n\\frac{\\partial C}{\\partial b^{(L)}}\\\\\n\\end{bmatrix}\n\\]\nWe can also do the same thing for the biases:\n\\[\n\\frac{ \\partial C_0 }{ \\partial b^{(L)} } = \\frac{ \\partial z^{(L)} }{ \\partial b^{(L)} } \\frac{ \\partial a^{(L)} }{ \\partial z^{(L)} } \\frac{ \\partial C_0 }{ \\partial a^{(L)} }\n\\] The first term on the RHS goes to 1, giving us \\[\n\\frac{ \\partial C_0 }{ \\partial b^{(L)} } = \\sigma ' \\left( z^{(L)} \\right) 2 \\left( a^{(L)} - y \\right)\n\\] If we have multiple neurons per layer and output layers \\(y_0\\) and \\(y_1\\), activations \\(a_0\\) and \\(a_1\\), and \\(w_{jk}^{(L)}\\) which is the weight of the \\(k^{\\text{th}}\\) neuron connecting the \\(j^{\\text{th}}\\) neuron, we get (again assuming MSE)\n\\[\nC_0 = \\sum_{j=0}^{n_{L-1}} \\left( a_j^{(L)} - y_j \\right)^2\n\\]\nwhich gives us the following for \\(z\\):\n\\[\nz_{j}^{(L)} = w_{j0}^{(L)} a_0^{(L-1)} + w_{j1}^{(L)} a_1^{(L-1)} + w_{j2}^{(L)} a_2^{(L-1)} + \\ldots + b_j^{(L)}\n\\]\nwhich results in\n\\[\na_j^{(L)} = \\sigma \\left( z_j^{(L)} \\right)\n\\]\n\n\n\nCitationBibTeX citation:@online{gregory2024,\n  author = {Gregory, Josh},\n  title = {Backpropagation {Calculus}},\n  date = {2024-05-06},\n  url = {https://joshgregory42.github.io/posts/2024-05-06-backprop-calc/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGregory, Josh. 2024. “Backpropagation Calculus.” May 6,\n2024. https://joshgregory42.github.io/posts/2024-05-06-backprop-calc/."
=======
    "objectID": "projects/aesthetics.html",
    "href": "projects/aesthetics.html",
    "title": "Aesthetics in Design Portfolio",
    "section": "",
    "text": "CU Boulder offers a class called “Aesthetics in Design”, offered at both the senior undergraduate and graduate levels. Throughout the course, I made several artifacts. What follows is a portfolio of the artifacts that I am most proud of. They can also be viewed on the website linked above, and they are copied over here for a more permanent home."
  },
  {
    "objectID": "projects/aesthetics.html#aesthetics-exporation---minimalism",
    "href": "projects/aesthetics.html#aesthetics-exporation---minimalism",
    "title": "Aesthetics in Design Portfolio",
    "section": "Aesthetics exporation - minimalism",
    "text": "Aesthetics exporation - minimalism\nI have always appreciated design that simplifies complex tasks in a beautiful (but simple) way. I feel as though a lot of design can be distracting, but thoughtful and simple design can serve as a way to allow someone to focus on what is important to them.\nThe minimalism movement as a philosophy had its start in the early 19th century in the United States by the likes of Ralph Waldo Emerson and Henry David Thoreau, who believed that “insight and enlightenment can be gained through solitude and simplicity”. The modern style of minimalism, however, was heavily inspired by the Bauhaus School in Germany during the early 20th century. It features a strong emphasis on basic shapes such as rectangles and circles. An example of this is in the Bauhaus logo itself, which was designed by Oskar Schlemmer in 1921.\n\n\n\nThe Bauhaus logo as designed by Oskar Schlemmer in 1921. Source\n\n\nWe can see that this logo shows a face in side profile, however it uses only lines, squares, and circles. The color palette as well is also simple, just black and white. These shapes can also be seen in architecture. The most logical example of this is perhaps the Bauhaus School itself in Dessau, Germany:\n\n\n\nThe Bauhaus building in Dessau, Germany. Source\n\n\nMuch like the Bauhaus logo, we can see that the building itself is composed almost entirely of simple colors (white, black, and gray with some red touches) as well as glass to allow for natural sunlight to enter the building.\nOne of the most influential figures in minimalism who brought the Bauhaus ideas into the mainstream is Deiter Rams. He is a German industrial designer that is known for his “Good design” principles that many minimalist (and non-minimalist) designers use to this day to assess the quality of their designs. Some of the rules he outlined are “is innovative”, “is unobtrusive”, “is as little design as possible”, and “is long-lasting”, among others. Some of his most famous works are the Vitsœ 620 Chair and the Braun ABW 30 clock, both shown below.\n\n\n\nThe Vitsœ 620 Chair, designed by Dieter Rams in 1962. Source\n\n\n\n\n\nBraun ABW30 clock designed by Dieter Rams in 1982. Source\n\n\nThe Vitsœ 620 Chair is able to have multiple assembled into a sofa. The two screws on the bottom disconnect the armrest and allow for two chairs to be screwed together to form a sofa, with the armrest being attached at the end of the two couches that are now screwed together. Furthermore, the chair itself is designed to be extremely easy to be assembled and is made out of high-quality components. Both of these are factors that show the long-lasting and simple design advocated for by Rams.\nThe Braun ABW30 clock is also extremely simple. Much like the Bauhaus logo, it only has two colors and the arms are of two distinct sizes, making it easy to tell the time.\nBoth Dieter Rams and the Bauhaus school were inspirations for Jony Ive, the former Chief Design Officer at Apple. Two examples of this are the original iPhone and the old MacBook Air model.\n\n\n\nRender of the original iPhone by Rafael Fernandez in 2017. Source\n\n\n\n\n\nThe original MacBook air from 2008. [Source](https://www.laptopmag.com/articles/old-macbook-air-worth-buying\n\n\nWhile both of these were groundbreaking and impressive machines when they were released, what I find most impressive and most telling about their aesthetic is that they still look good, 16 years later. To me, that is what good minimalism should do. Not only should the designs be made out of quality materials (i.e. metal and glass), but also be timeless."
  },
  {
    "objectID": "posts/2024-05-06-thesis-resources/index.html",
    "href": "posts/2024-05-06-thesis-resources/index.html",
    "title": "MS Thesis Resources",
    "section": "",
    "text": "This is a collection of resources I’m using for my MS Thesis concerning vision transformers (VTs) applied to brain vasculature. This page will probably be updated over time.\n\nML Resources\nCoding problems\n3Blue1Brown Deep Learning YouTube playlist\nA Mathematical Framework for Transformer Circuits\nCreating a Transformer from Scratch\nAndrej Karpathy GPT\n\n\n\n\nCitationBibTeX citation:@online{gregory2024,\n  author = {Gregory, Josh},\n  title = {MS {Thesis} {Resources}},\n  date = {2024-05-06},\n  url = {https://joshgregory42.github.io/posts/2024-05-06-thesis-resources/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nGregory, Josh. 2024. “MS Thesis Resources.” May 6, 2024. https://joshgregory42.github.io/posts/2024-05-06-thesis-resources/."
  },
  {
    "objectID": "projects/aesthetics.html#upcycle-aesthetic-organic-shapes-and-knife-woodblocks",
    "href": "projects/aesthetics.html#upcycle-aesthetic-organic-shapes-and-knife-woodblocks",
    "title": "Aesthetics in Design Portfolio",
    "section": "Upcycle aesthetic: organic shapes and knife woodblocks",
    "text": "Upcycle aesthetic: organic shapes and knife woodblocks\nThe knife storage solution in my apartment is currently precarious, to put things lightly. All of our knives are currently in a drawer in random orientations with knives of various lengths. They also happen to be somewhat near other cutlery, like forks and spoons, which means that it is a small miracle all of my roommates and I haven’t seriously cut ourselves. I’m honestly too embarrassed to take an actual picture of our knife/cutlery drawer, but it looks like a messier version of this picture:\n\n\n\nA semi-messy knife drawer. Source\n\n\nThis problem led me to thinking about creating a knife block to store my knives. Not only would it solve a genuine problem I have, but it would also allow me to explore something that could have subtle design cues that I like while also being sustainable. The sustainable aspect of this knife block is the sourcing of the wood itself. I have a friend who works extensively with wood on various projects. As such, he has a significant amount of scrap wood that would otherwise be thrown away. Instead of discarding it, he and I discussed using the various scraps of wood to create a wood block. This would create a unique, one of one pattern for the knife block. Not only would this look different compared to other knife blocks, but it would also put a significant amount of wood to use that would otherwise be discarded.\nThe design of the knife block itself is something that I have given considerable thought to, even if some of the design changes that I want to make to it are subtle. There was an interview with Craig Federighi, Apple’s head of software design, where he mentions that human faces have an lot of curves, so it makes sense to have the software reflect that. This can also be seen in other Apple-designed areas, such as the staircase railing design in Apple Park:\n\n\n\nStair design in Apple Park. Source\n\n\nMost staircases have sharp edges, but these have a radius of curvature and there are very few harsh edges. This desire to erase perpendicular edges and create a knife block that looks organic is something that I am going to attempt to pull off.\nThe most differing aspect of my knife block will be that it will be made out of a single sanded piece of wood sandwiched together. Most (if not all) knife blocks are actually made with two pieces of wood, as can be seen here:\n\n\n\nA concentional wooden knife block. Source\n\n\nMy design would get rid of this line, and instead feature one continuous line, from the top of the knife block to the waterfall edge to the base. The block would also not have the sharp angles that can be seen at the rear of this knife block, instead continuing the line from the rear until it meets the table that it rests on. The cut-outs for each of the knives, as well, would be rectangles with rounded corners instead of rectangles with perpendicular vertices. Again, while all of these design changes are subtle, I believe that (at least to me), this would increase the beauty of something as a knife block that most people don’t give a second thought to."
  },
  {
    "objectID": "projects/aesthetics.html#upcycle-design-report",
    "href": "projects/aesthetics.html#upcycle-design-report",
    "title": "Aesthetics in Design Portfolio",
    "section": "Upcycle design report",
    "text": "Upcycle design report\n\nDesign inspiration\nAs a brief recap, my Upcycle project is to use scrap/reused wood to create a knife block that is made to look like it is made out of a single block of wood as much as possible. The inspiration for a knife block specifically came from the fact that the knives I currently have are stored in a few drawers in various orientations, which scares me and isn’t really safe. I also think it would be nice to have a knife block that I can add to as I get more knives, since right now I only have a few.\n\n\n\nA concentional wooden knife block. Source\n\n\nThe image above is what most knife blocks currently look like. The main element that I wanted to change was how this knife block looks like there are two different pieces of wood glued together; one at the front and one at the rear. I don’t like this unnecessary complexity, and believe that a minimalist aesthetic applied to this would involve both of these pieces of wood being infused into one piece, creating an uninterrupted line on the sides. This comes from some of my more organic inspirations, such as the minimalism from Dieter Rams and Jony Ive at Apple Park, an example of which is the organic shape of their stairs:\n\n\n\nStair design in Apple Park. Source\n\n\n\n\nFabrication process\nThe fabrication process started by first using a knife block that a friend already had and gathering some dimensions from there in order to gain some intuition on the spacing:\n\n\n\nKnife block that was used for dimensioning mine.\n\n\n\n\n\nSketch of the knife block dimensions before fabrication began.\n\n\nOnce the dimensions were gathered, the raw materials were obtained. A long scrap piece of oak was found and was cut into several pieces and glued together to create the overall frame of the knife block. You can see this in the following picture. If you look closely, you can see what looks like four wooden squares glued together (note the difference in grain direction, that’s the giveaway):\n\n\n\nGlued knife block before the initial cuts.\n\n\nThe original plan was to cut several incisions similar to the one in the image above on either side of the wood and then frame it with more scrap wood. That was all fine and dandy until this happened:\n\n\n\nSide view of the knife block after making some of the knife holes.\n\n\nIt might be hard to see, but the wood decided to split. I was going too fast and things started breaking. Not only that, the entire knife block split in half due to some of the wood glue not being able to set correctly. This meant that the entire thing had to be taken apart, sanded down, and then glued again. With a lot of clamps. Like more clamps than you’ve probably seen on something:\n\n\n\nFixing the knife block after it decided to split itself apart.\n\n\nThe crazy number of clamps were used to ensure that there was enough even pressure throughout the wood to make sure that the glue would create a solid bond with the wood. This part is really key, since wood is porous and the pressure must be applied over the top of parallel wood grains. Otherwise the glue fails and the entire thing falls apart (again). This set overnight so that the same mistake wasn’t repeated, and thankfully the glue dried and held.\n\n\n\nA joiner being used to create the angles and flat edges.\n\n\nAnother part of this experience was working with a friend who is extremely experienced in woodworking. He helped me think through the design, showed me the tools we would need to use and how to operate them, and overall his thought process in looking at a completed product, deconstructing it, and then modifying it to the aesthetic that I was going for. Furthermore, I learned a lot about the sheer amount of time making something like this takes. It was about 12 hours of work over two whole days, not counting the time for the glue to dry, which would add another several hours. The end product, after it was glued and sanded, is something I’m pretty happy with:\n\n\n\nTop view\n\n\n\n\n\nIsometric view\n\n\n\n\n\nSide view\n\n\n\n\n\nRear view\n\n\n\n\n\nSteak knife view. The lighter piece of wood on the top of the front face is made out of birch.\n\n\n\n\n\nTop view without knives.\n\n\n\n\n\nIt holds knives!\n\n\nThe sanding process was also extremely satisfying. The wood got softer to the touch than I even knew possible, and I also found a piece of birch wood to use for the top portion of the front (that’s the lighter band in the front view). I think it adds a nice splash of color, and when I go to stain the knife block it will add a nice contrast to the rest of the project.\nWhile I didn’t get the rounded corners and edges I was going for by the time this project was due, I am thinking of going in and adding them. That being said, I did get some feedback from some members of my pod that they actually liked the edges and how it lends to an angular feature to the knife block, similar to the angular features from the side. I hadn’t really thought of this, but am going to keep the knife block on my counter for a few weeks as it is and see if I see it the same way, or if I do end up wanting to curve the edges.\nI am also going to go in and stain the entire knife block. Professer Hertzberg recommended walnut oil, which seems really interesting and I think would add a nice color to the knife block and make it appear more finished.\nPerhaps the best part was seeing the knife block actually hold knives! I had put it in so much work, and I was so nervous that the measurements would be correct, but for some reason the holes would either be too big or too small. But seeing the knives slide right in was really satisfying. This is definitely something I am going to use for (hopefully) years to come, and will hopefully get a lot of love :)"
  },
  {
    "objectID": "projects/aesthetics.html#main-project-plans",
    "href": "projects/aesthetics.html#main-project-plans",
    "title": "Aesthetics in Design Portfolio",
    "section": "Main project plans",
    "text": "Main project plans\nI think my personal aesthetic is something maybe related to timeless design. My mom’s side of the family is from Italy, and as such, the places there have influenced me significantly. Furthermore, being born in London has also influenced me in many other ways. I am also a massive fan of Ferrari’s Formula One team and the company as a whole. I believe that Ferrari is able to combine art and engineering unlike most other companies today. There are a few others, like Pagani Automobili, Apple, and IKEA more from a purely design perspective. However, none of those companies have the racing pedigree or command the same amount of passion that Ferrari does. One of their cars that comes to mind for me is the Ferrari P4, which was an endurance race car that competed in the late 1960s.\n\n\n\nThe Ferrari P4. Source\n\n\nAnother major aspect of my life that has influence over the aesthetic is that from the music world. I’ve been playing the cello for over 10 years, and because of that I’ve spent a lot of time in symphony rehearsals and with my own instrument specifically. I’ve always loved how it looks, and while it’s hard to describe in text, the comfort I get from playing mine is sort of like hugging a really close friend. It feels familiar, and to me, the cello as an instrument still looks good, hundreds of years after its design.\n\n\n\nStradivari cello from Museo di Violino in Verona, Italy. Source\n\n\nI have also been heavily inspired by nature, specifically growing up in the Pacific Northwest in Portland, Oregon. One of my favorite places to visit there are the Portland Japanese Gardens, which are widely considered to be the most accurate Japanese gardens in the world outside of Japan. The gardens themselves are part of a 400-acre park in Portland that overlooks the city and is reserved for people hiking, biking, and otherwise enjoying Portland. No development is allowed on it by law, which is something I’ve always appreciated. Even though I have been going to the gardens for over a decade, they have always evoked a sense of peace and connectedness inside of me. It’s pretty impressive when I think about it; the design of the gardens hasn’t fundamentally changed since its construction, but it still looks amazing no matter the weather and still makes me feel something very few other places can.\n\n\n\nPortland Japanese Garden. Source\n\n\nOne thing I have always appreciated about the Portland Japanese Garden is how the entrance and existing architecture blends in so seamlessly with the nature surrounding it. I absolutely love the design of the handrails. They appear to be one long, connected piece of steel that is joined by glass plates. And the concrete footpaths make you feel grounded as you walk up them. It’s very hard to explain over text, but it does make you feel as though you are floating in a forest.\nAs far as what my main project plan is, I’m honestly still a bit unsure. I was thinking about making a coaster for my mugs, since I still don’t have one for my desk, but want one that is able to fit multiple diameters of cup within one coaster. It might be an interesting exercise in creating something that is functional, timeless in design, but also minimalist. How much could I actually take away from a coaster and still have it be a coaster? Is adding something that is necessary even a good idea? Starting with something simple like a coaster might be a good exercise in minimalism design from the ground up. It’s something I’ve never done before and might lead to something interesting :)"
  },
  {
    "objectID": "projects/aesthetics.html#design-preview-report---coaster",
    "href": "projects/aesthetics.html#design-preview-report---coaster",
    "title": "Aesthetics in Design Portfolio",
    "section": "Design preview report - coaster",
    "text": "Design preview report - coaster\n\nInitial thoughts\nMy initial inspiration for this project came from a desire to have a coaster that fits cups of different dimensions. I have many various mugs and glasses, but have never had a single coaster that was able to hold them all. I’ve definitely had coasters in the past that are able to hold cups of a single dimension, but every other cup dimension has felt forced or not right. I found this picture on Reddit that sums up how it makes me feel pretty well:\n\n\n\nCup holder adapted to a Mini to hold a larger cup. Source\n\n\nThis is the kind of frustrating thing that I have seen about cup holders. Why are they so friggin small?? Someone shouldn’t need to 3D print something to be able to fit a regular coffee cup in it.\nAs far as aesthetics go, I have been exploring minimalism for most of this semester, and really wanted to try to incorporate more curved edges into this design. In my Upcycle project I wasn’t able to include as many rounded corners as I would have liked due to the fabrication process, but I thought that this project would be a good opportunity to do that.\n\n\nInspirations\nThis is drawing a bit from two posts ago, but I figured it was worth repeating here. I have always loved the Portland Japanese Gardens, specifically how their design has always felt timeless. I’ve been going there for over a decade and always feel at peace and rejuvenated by visiting.\n\n\n\nPortland Japanese Garden. Source\n\n\nIn a similar vein, I own a mug by Kinto Japan that is made using local artisan techniques that have been around for hundreds of years. The mug itself is an absolute joy to hold, even though it looks so simple:\n\n\n\nCoffee mug by Kinto Japan. Source\n\n\nThese are what formed the basis of my design aesthetics. I really wanted to try and push myself to use as few 90 degree angles as possible, focusing as much on curves as I possibly could and using as little material as possible.\n\n\nOpposite aesthetics\nTwo main opposite aesthetics I found were that of the Baroque and steampunk styles. I was struggling to come up with good sketches for coasters that would follow those respective aesthetics, so I turned to DALL-E for the first time, and it came up with some designs that captured what I had in my head exceptionally well.\nhold, even though it looks so simple:\n\n\n\nA coaster in the baroque style from DALL-E.\n\n\nThis really captures the opposite aesthetic of what I am going for. The main difference is how ornate and detailed the coaster is. There are a seemingly infinite number of flourishes and ornamentations present. My eyes are drawn towards the main five “nodules”, but then are quickly drawn towards what are surrounding them as well as the cherubs that are shown. It all becomes really overwhelming, and then my eye eventually realizes that the entire thing is made out of marble, like most sculptures of the time.\nThe other aesthetic is steampunk, which I also used DALL-E for:\n\n\n\nA coaster in the steampunk aesthetic generated by DALL-E.\n\n\nWhile this is also a very busy design, it is busy in a different way than the Baroque style one before. Instead of focusing on marbled flourishes, this focuses on using gears and mechanisms to create a sense of busyness and clutter. On the one hand, I do like imagining all of the different things that these gears could power (it sort of looks like a mechanical watch face), but on the other, I really dislike how much clutter there is, especially since I know that none of it is functional.\nThese two opposite aesthetics didn’t really influence my design that much, mostly since I knew that I wanted something that was simple and pleasing to both look at and use.\n\n\nSome initial renders\nWhat follows are some very rough renders of my thoughts. The base of the coaster is able to accommodate mugs that are 50mm in diameter, increasing by increments of 5mm until it reaches 80mm. It sort of looks like a tiered cake, and to be honest I’m not sure if I like the design.\n\n\n\nFirst coaster render from SolidWorks.\n\n\n\n\n\nA close-up render of the curves.\n\n\nThe one thing I do like about the design currently is the color, Most of the things on my desk are dark (the desk itself is black), so having something that is white would not only stand out from an aesthetics perspective, but would also mean that I could see it from my peripheral vision and could place a cup down inside of it entirely by feel without needing to look away from what I’m doing or working on. The number of times that I have tried to do that and nearly spilled something hot on myself and/or my laptop is too high. And I don’t think I’m the only one. So hopefully making it out of a color that is pretty much the opposite from everything else will help significantly with that problem.\n\n\nNext steps\nFabrication is going to be 3D printing, most likely out of white PLA with the option to make this out of white resin using a FormLabs 3D printer, which I have already used extensively on my senior design project, so it’s a machine I feel comfortable with. Next steps are roughly the following:\nNext week (18 March): Iterate on the design. Take some more measurements of different cups and test fits.\n1 April: Print some prototypes and consider adding in flexible resin for a more streamlined up experience\n8 April: Try seeing how a curved bottom might work. Could the bottom be filleted somehow to get rid of another curve? What would happen if it tilted with something inside of it? Can you put a cup in there blindfolded just by feel?\n15 April-end: Extra time since something is going to go wrong, I’m going to want to change something, or something I haven’t thought of happens."
  },
  {
    "objectID": "projects/aesthetics.html#final-design-progress",
    "href": "projects/aesthetics.html#final-design-progress",
    "title": "Aesthetics in Design Portfolio",
    "section": "Final design progress",
    "text": "Final design progress\nMy design so far has had to be completely re-designed. Originally, I was planning on creating something that looked like this:\n\n\n\nFirst coaster render from SolidWorks.\n\n\nHowever, after some really helpful critiques from my pod members, I have decided to redesign my coaster to feature a spring mechanism instead of the multi-tiered level design as shown above. I’ve created some preliminary sketches for this, attached below.\n\n\n\nNew sketch featuring the spring mechanism.\n\n\nTo ensure that the spring would be able to handle one of my large mugs, I took the two most common mugs, filled them with water, and weighed them with a scale:\n\n\n\nilly cappuccino mug filled with water.\n\n\n\n\n\nKinto Japan mug filled with water.\n\n\nTo ensure that the springs would compress adequately, I took some preliminary measurements and calculated the spring constant needed, assuming that I have four springs on the top/bottom and left/right sides of the spring plate on my coaster:\n\n\n\nMug measurements.\n\n\n\n\n\nSpring constant calculations.\n\n\nI have since purchased some springs that roughly meet these specifications. I am also currently working on some software that will give me a range of acceptable spring constants that will work given the spring dimensions. Amazon sells several cheap spring sets, so I’m going to see if the dimensions from some of those springs will work well for my coaster.\nMy immediate next step is to create an updated CAD model, included with the springs. I will also be including slots on the sides of the coaster itself to allow for the mug’s handle to pass through."
  },
  {
    "objectID": "projects/aesthetics.html#final-report-part",
    "href": "projects/aesthetics.html#final-report-part",
    "title": "Aesthetics in Design Portfolio",
    "section": "Final report part",
    "text": "Final report part"
  },
  {
    "objectID": "projects/aesthetics.html#final-report-part-one",
    "href": "projects/aesthetics.html#final-report-part-one",
    "title": "Aesthetics in Design Portfolio",
    "section": "Final report part one",
    "text": "Final report part one\n\nThe minimalist cupholder/coaster\nWhile I have covered this in some of my other posts, I think it would be a good idea to talk about the motivation behind the project that I have been working on. As of now, the cupholder on my desk is a warranty booklet for a Seagate hard drive. It’s literally a small book that you usually flip through to see if there are any stickers inside. While I could have easily gotten any kind of cupholder and solved my problem, I found a problem similar to this one:\n\n\n\nCup holder adapted to a Mini to hold a larger cup. Source\n\n\nThis kind of design really pisses me off. Why, in all that is holy, can a regular coffee cup not fit into the cup holder of a car that made it to production? It irritates me like nothing else. I wanted to design something that would allow for a cup to fit inside of it and feel as though it was made for that cupholder.\n\n\nAesthetics inspirations\nThroughout this course, I have taken a lot of inspiration from the likes of the Bauhaus movement as well as Dieter Rams and the industrial design group at Apple, specifically under Jony Ive. An example of this that I believe really captures this aesthetic is the design of the stair railings at Apple Park:\n\n\n\nStair design in Apple Park. Source\n\n\nI absolutely love how curvy these stairs are, and really wanted to try and challenge myself to create something that is minimalist while at the same time being functional and containing a lot of organic curvature. The curvature, to me, feels organic in a way that is refreshing from a life that (especially for an engineering student) is filled of right angles, harsh environments (looking at you, engineering center designers), and human manufacture. I think organic shapes are grounding and a pleasing contrast to our everyday experiences.\n\n\nFinal renders\n\n\n\nFinal render with an IKEA mug, CAD of which was taken from here.\n\n\n\n\n\nFinal render with no mug.\n\n\nThe details of my design will be talked about in much greater detail in the following post (it had to be completely redesigned), but I believe that I have made something that I am proud of. The springs on the bottom of the artifact allow for the mug to be filled with water entirely (I did the math for spring constants for various mugs I own filled with water) and to slot into any of the four slots on the perimeter of the artifact. The white color was chosen so that I can place it on my black desk and see the stark contrast out of the corner of my eye without needing to take away my vision. The ergonomics were also something I gave a lot of thought to; the filleted edges on the top are for aesthetic reasons (organic shapes), however they do provide a bit of tactile feedback, guiding the handle of the mug I’m using into the slot without me needing to look directly at the artifact to align the mug properly, one of the main design goals I had for this project.\n\n\nFinal assembly\n\n\n\nDraft 3D print used for checking spring and cup alignment.\n\n\n\n\n\nFinal assembly top/mini exploded view (springs removed).\n\n\n\n\n\nArtifact shell side view."
  },
  {
    "objectID": "projects/aesthetics.html#final-report-part-two",
    "href": "projects/aesthetics.html#final-report-part-two",
    "title": "Aesthetics in Design Portfolio",
    "section": "Final report part two",
    "text": "Final report part two\n\nThings went quite wrong\nInitially, my design was going to be simple. Really simple, and, quite honestly, too simple. Initially, I was going for something that held cups that looked like this:\n\n\n\nFirst coaster render from SolidWorks.\n\n\nThe original plan was to improve upon this design, adding room for the handles of various cups. While the design was pure (both in the color but in the fact that each dimension was a multiple of 5), it didn’t have a dynamic component. During my design review with my pod, I realized that I would need to go back to the drawing board. I was honestly a bit upset at myself for feeling like I had made things too simple. But then I took a step back and realized that one of the more difficult things about minimalism that I hadn’t really considered was how fine the line can be between something that is aesthetically pleasing and minimalist, and something that is too simple and is just crap. I had made the latter. And while that was frustrating, looking back on the experience, I think it was a good one. At any rate, I was back to the drawing board. This made my eventual schedule look significantly different than what I had planned:\n\nOriginal plan\n1 April: Print some prototypes and consider adding in flexible resin for a more streamlined up experience\n8 April: Try seeing how a curved bottom might work. Could the bottom be filleted somehow to get rid of another curve? What would happen if it tilted with something inside of it? Can you put a cup in there blindfolded just by feel?\n15 April - end: Extra time since something is going to go wrong, I’m going to want to change something, or something I haven’t thought of happens.\n\n\nWhat actually happened\n1 April - 8 April: Redesign the entire thing. From the ground up. Focusing on tactile feel and ergonomics. Incorporate a spring mechanism and maybe some kind of damping system.\n8 April - 15 April: Take measurements of various cups and mugs, both filled and empty. Write some code to calculate what the spring dimensions should be, the spring constants, etc. Start modeling work in CAD.\n15 April - 22 April: Get springs ordered and delivered, test fits and different colors.\n22 April - 29 April: Final assembly and minor tweaks\n\n\n\nThe redesign\nThe next immediate step was obviously to redesign the part. From the feedback with my pod, I realized that I wanted to include some springs in my design so that the cup’s handle would slide into and out of little slots. Here’s what some of the renders of that looked like:\n\n\n\nArtifact side view\n\n\n\n\n\nFinal render with an IKEA mug, CAD of which was taken from here.\n\n\nThe curved/filleted edges on the top and bottom of the outer “shell” is so that I could place the mug in roughly the right spot and then feel the entrance to the slots without directly needing to look at the artifact itself. The white and brown colors were used to try and get a sense of the contrast between the two, since my desk is darker and the contrast in the peripheral vision is something that was also important for this design.\n\n\nThe springs\nTo get an idea of the spring specifications, I used a coffee scale to weigh some of my favorite mugs (especially the biggest/heaviest ones) to gain an understanding of the masses I was dealing with. See images of the mugs above. Once I had these masses, I made a free-body diagram with the masses and used some of the spring equations from Component Design to inform my desired spring dimensions and their respective spring constants.\n\n\nManufacturing\nThe shell and plate were both 3D printed to test fits and tolerances between parts. Incredibly, everything fit together well enough that I decided to keep the dimensions the same for the final version.\n\n\n\nTop view with springs.\n\n\n\n\n\nSide view with springs.\n\n\n\n\n\nTop part on.\n\n\n\n\n\nCup in the assembly!\n\n\n\n\n\nFinal prints in white.\n\n\nOne thing I found was that while the springs I ordered fit the dimensions of the holes I provided, they were too weak. So I had some other (smaller) springs ordered that fit concentrically to provide more stiffness.\n\n\nTakeaways\nOverall, I would say the main takeaway from this project is that minimalism and ergonomics can be challenging if you become lazy or aren’t as focused as you should be. That’s not necessarily a bad thing as long as you learn from it, but it was not something I had expected to hit me as hard as it did. All of that being said, being in a class environment like this and able to recover from this mistake, and also feel comfortable taking the risk to make this mistake, was something I’m thankful for."
  },
  {
    "objectID": "projects/aesthetics.html#current",
    "href": "projects/aesthetics.html#current",
    "title": "Aesthetics in Design Portfolio",
    "section": "Current",
    "text": "Current"
  },
  {
    "objectID": "projects/aesthetics.html#final-version",
    "href": "projects/aesthetics.html#final-version",
    "title": "Aesthetics in Design Portfolio",
    "section": "Final version",
    "text": "Final version\nHere’s the knife block stained an in use in my kitchen:\n\n\n\nStained and completed knife block."
  },
  {
    "objectID": "posts/2024-06-04-undergrad-advice/index.html",
    "href": "posts/2024-06-04-undergrad-advice/index.html",
    "title": "Advice for Undergrads",
    "section": "",
    "text": "After recently graduating from my undergraduate program in mechanical engineering at CU Boulder, I thought of reflecting a bit and sharing some things that I learned besides just the schooling. Hopefully it’s helpful for others at any point on their journey :)"
  },
  {
    "objectID": "posts/2024-06-04-undergrad-advice/index.html#the-schools-non-academic-qualities-matter-too",
    "href": "posts/2024-06-04-undergrad-advice/index.html#the-schools-non-academic-qualities-matter-too",
    "title": "Advice for Undergrads",
    "section": "The school’s non-academic qualities matter too",
    "text": "The school’s non-academic qualities matter too\nThis is one that most people seem to not think about as much as they should. Put simply, a school’s academics mean nothing if you hate the environment. You could be going to the best university in the world for your field, but if you are demeaned by the faculty, made to feel like an outsider, or just generally don’t like the environment, the academics don’t matter. You won’t be able to take full advantage of them because mentally you will be distracted by how much you hate dealing with other students, the horrible weather, etc. For me, the academics are more of a check box. Yes, they should be strong. But I don’t think they should be a majority of the discussion. Most schools will brag about their academics, but I would ask more about mentorship opportunities. How easy is it for you to get involved in research (if you want to, of course)? How available are professors? Also look at the department as a whole. CU’s Mechanical Engineering Department is incredible. Yes, there are a lot of students, but the department does a great job at making it feel small and connected. Many professors know my by name and we frequently talk in the hallways about anything and everything. I’ve emailed quite a few of them being both confused but curious about something we discussed in class, and 100% of them were willing to give hours of their time to share their passion with me. Those kinds of interactions are meaningful and go a long way to making someone feel welcome.\nAnother major aspect to consider is the weather and surrounding city and your preferences there. At OSU, I was an hour and a half away from the nearest major city. OSU is in Corvallis, Oregon, which is a pretty small college town. Really the only thing I found to do there was to drink. For some people that’s fine, but for me I wanted to do other things.\n\n\n\nOregon State University campus in Corvallis, OR. Source\n\n\n\n\n\nUniversity of Colorado Boulder campus in Boulder, CO. Source\n\n\nFor me, Boulder as a city was exactly what I was looking for. As a runner, access to easy running trails was a huge bonus (a path I frequently ran on was literally right outside my door), but Pearl Street and nearby places to visit was also amazing. The community felt alive, and there were other things to do and see besides the college campus. Denver was also a half hour away, so if I wanted to go into a major city for whatever reason, that was something I could easily do as well.\nIn short, it is always a good idea to live in a place that has options so that you don’t limit yourself accidentally. There are also some things you don’t realize you want until you’re experiencing them. For me, one of the more surprising things about moving to Boulder was its food scene. In Corvallis, there aren’t many restaurants that offer a diverse set of cuisine. Boulder is the opposite of that. I could have excellent Indian food, Nepalese, Korean, and many others whenever I felt like it. Again, that wasn’t something that I thought about a lot while I was in Corvallis, but was something I realized I loved when transferring to CU."
  },
  {
    "objectID": "posts/2024-06-04-undergrad-advice/index.html#follow-your-interests",
    "href": "posts/2024-06-04-undergrad-advice/index.html#follow-your-interests",
    "title": "Advice for Undergrads",
    "section": "Follow your interests",
    "text": "Follow your interests\nSaid another way, be wary of becoming pigeonholed. Academia thrives on people having deep specializations, which for some people is exactly what they want. It’s important to recognize that academia will tend to push you towards a narrow specialization unless you do something about it.\nFor me, that was in rocketry. My background in computational fluid dynamics (CFD) was originally applied to liquid-fuelled rocket engines. However, at the onset of the COVID pandemic, I decided that I wanted to use my engineering skills to help people in a more direct way, so I switched to a lab that focuses on CFD applied to the human body. I didn’t know anything about biomedical engineering, but knew that fundamentally, the math behind fluid mechanics applied to rocket engines and the body would be the same. I let my interest in helping people from a biomedical perspective carry me through many of the difficult times of having absolutely no idea what was going on.\nIn retrospect, being able to switch fields like this is extremely important. Having the flexibility to be cross-disciplinary is something that I have found I really enjoy, although your mileage may vary."
  },
  {
    "objectID": "posts/2024-06-04-undergrad-advice/index.html#no-one-gets-through-it-alone",
    "href": "posts/2024-06-04-undergrad-advice/index.html#no-one-gets-through-it-alone",
    "title": "Advice for Undergrads",
    "section": "No one gets through it alone",
    "text": "No one gets through it alone\nThis might be something more specific to CU, but I suspect that it isn’t. Everyone here wants to help everyone else. Each person who gets past freshman year in this program knows that it’s gruelling. For most people, that brings out the best in them. 99% of people want to help everyone else succeed. If you’re struggling, there is a culture of asking for help. If you struggle on an exam but know someone who did well, I’m confident that 99% of the time they will be more than happy to meet with you and talk things over. I’m convinced that it’s pretty much impossible to get through an engineering degree by yourself without some kind of outside help."
  },
  {
    "objectID": "posts/2024-06-04-undergrad-advice/index.html#relationships-and-friends",
    "href": "posts/2024-06-04-undergrad-advice/index.html#relationships-and-friends",
    "title": "Advice for Undergrads",
    "section": "Relationships and friends",
    "text": "Relationships and friends\nI had a frew relationships during my time as an undergraduate, and to answer your question, yes it can happen. And yes, it’s hard. If you’re going to do one, making sure that your partner knows that you’re going to be working a lot is really key. Communication, as much as people say is important, becomes critical. It will make or break the relationship. I also found it helpful to schedule dates that I didn’t move and to try and check in about once a week with my girlfriend to see how the relationship was going. Everything was on the table and each person had the space to bring up anything they wanted, good or bad.\nAll of this being said, both people need to be willing to work through difficult times. The honeymoon phase wears off. Surprise assignments come up. I once was spending time with a girl I was dating and we were eating dinner together after a long week when someone on a group project texted me saying that he couldn’t finish the code. He had a week and had written three lines. It was due that night. So I had to finish it myself. Having a partner that understands that things like that come up is really important, but also as equally important is being able to communicate openly and honestly with each other if it ever becomes too much."
  },
  {
    "objectID": "posts/2024-06-04-undergrad-advice/index.html#values",
    "href": "posts/2024-06-04-undergrad-advice/index.html#values",
    "title": "Advice for Undergrads",
    "section": "Values",
    "text": "Values\nBecause the program is so rigorous, you’ll quickly learn the value of hard work, dedication, not giving up (both on yourself and others), thinking through problems, and learning how to balance life and school (as best as you can, at least). The value placed on hard work is one I’ve found to be specific in STEM, although it could definitely be present in other majors. Because everyone works so hard, you will go through hell and back multiple times with your friends. And you’ll all be stronger because of it and have a deeper relationship with each other. Those people will go to the ends of the earth for you, and I know that for the friends I have made through this program, if I were to need any of them for any kind of emergency, they would drop everything and help me. And they know I would do the same for them. No questions asked."
  },
  {
    "objectID": "posts/2024-06-05-undergrad-advice/index.html",
    "href": "posts/2024-06-05-undergrad-advice/index.html",
    "title": "Advice for Undergrads",
    "section": "",
    "text": "After recently graduating from my undergraduate program in mechanical engineering at CU Boulder, I thought of reflecting a bit and sharing some things that I learned besides just the schooling. Hopefully it’s helpful for others at any point on their journey :)"
  },
  {
    "objectID": "posts/2024-06-05-undergrad-advice/index.html#the-schools-non-academic-qualities-matter-too",
    "href": "posts/2024-06-05-undergrad-advice/index.html#the-schools-non-academic-qualities-matter-too",
    "title": "Advice for Undergrads",
    "section": "The school’s non-academic qualities matter too",
    "text": "The school’s non-academic qualities matter too\nThis is one that most people seem to not think about as much as they should. Put simply, a school’s academics mean nothing if you hate the environment. You could be going to the best university in the world for your field, but if you are demeaned by the faculty, made to feel like an outsider, or just generally don’t like the environment, the academics don’t matter. You won’t be able to take full advantage of them because mentally you will be distracted by how much you hate dealing with other students, the horrible weather, etc. For me, the academics are more of a check box. Yes, they should be strong. But I don’t think they should be a majority of the discussion. Most schools will brag about their academics, but I would ask more about mentorship opportunities. How easy is it for you to get involved in research (if you want to, of course)? How available are professors? Also look at the department as a whole. CU’s Mechanical Engineering Department is incredible. Yes, there are a lot of students, but the department does a great job at making it feel small and connected. Many professors know my by name and we frequently talk in the hallways about anything and everything. I’ve emailed quite a few of them being both confused but curious about something we discussed in class, and 100% of them were willing to give hours of their time to share their passion with me. Those kinds of interactions are meaningful and go a long way to making someone feel welcome.\nAnother major aspect to consider is the weather and surrounding city and your preferences there. At OSU, I was an hour and a half away from the nearest major city. OSU is in Corvallis, Oregon, which is a pretty small college town. Really the only thing I found to do there was to drink. For some people that’s fine, but for me I wanted to do other things.\n\n\n\nUniversity of Colorado Boulder campus in Boulder, CO. Source\n\n\n\n\n\nOregon State University campus in Corvallis, OR. Source\n\n\nFor me, Boulder as a city was exactly what I was looking for. As a runner, access to easy running trails was a huge bonus (a path I frequently ran on was literally right outside my door), but Pearl Street and nearby places to visit was also amazing. The community felt alive, and there were other things to do and see besides the college campus. Denver was also a half hour away, so if I wanted to go into a major city for whatever reason, that was something I could easily do as well.\nIn short, it is always a good idea to live in a place that has options so that you don’t limit yourself accidentally. There are also some things you don’t realize you want until you’re experiencing them. For me, one of the more surprising things about moving to Boulder was its food scene. In Corvallis, there aren’t many restaurants that offer a diverse set of cuisine. Boulder is the opposite of that. I could have excellent Indian food, Nepalese, Korean, and many others whenever I felt like it. Again, that wasn’t something that I thought about a lot while I was in Corvallis, but was something I realized I loved when transferring to CU."
  },
  {
    "objectID": "posts/2024-06-05-undergrad-advice/index.html#follow-your-interests",
    "href": "posts/2024-06-05-undergrad-advice/index.html#follow-your-interests",
    "title": "Advice for Undergrads",
    "section": "Follow your interests",
    "text": "Follow your interests\nSaid another way, be wary of becoming pigeonholed. Academia thrives on people having deep specializations, which for some people is exactly what they want. It’s important to recognize that academia will tend to push you towards a narrow specialization unless you do something about it.\nFor me, that was in rocketry. My background in computational fluid dynamics (CFD) was originally applied to liquid-fuelled rocket engines. However, at the onset of the COVID pandemic, I decided that I wanted to use my engineering skills to help people in a more direct way, so I switched to a lab that focuses on CFD applied to the human body. I didn’t know anything about biomedical engineering, but knew that fundamentally, the math behind fluid mechanics applied to rocket engines and the body would be the same. I let my interest in helping people from a biomedical perspective carry me through many of the difficult times of having absolutely no idea what was going on.\nIn retrospect, being able to switch fields like this is extremely important. Having the flexibility to be cross-disciplinary is something that I have found I really enjoy, although your mileage may vary."
  },
  {
    "objectID": "posts/2024-06-05-undergrad-advice/index.html#no-one-gets-through-it-alone",
    "href": "posts/2024-06-05-undergrad-advice/index.html#no-one-gets-through-it-alone",
    "title": "Advice for Undergrads",
    "section": "No one gets through it alone",
    "text": "No one gets through it alone\nThis might be something more specific to CU, but I suspect that it isn’t. Everyone here wants to help everyone else. Each person who gets past freshman year in this program knows that it’s gruelling. For most people, that brings out the best in them. 99% of people want to help everyone else succeed. If you’re struggling, there is a culture of asking for help. If you struggle on an exam but know someone who did well, I’m confident that 99% of the time they will be more than happy to meet with you and talk things over. I’m convinced that it’s pretty much impossible to get through an engineering degree by yourself without some kind of outside help."
  },
  {
    "objectID": "posts/2024-06-05-undergrad-advice/index.html#relationships-and-friends",
    "href": "posts/2024-06-05-undergrad-advice/index.html#relationships-and-friends",
    "title": "Advice for Undergrads",
    "section": "Relationships and friends",
    "text": "Relationships and friends\nI had a frew relationships during my time as an undergraduate, and to answer your question, yes it can happen. And yes, it’s hard. If you’re going to do one, making sure that your partner knows that you’re going to be working a lot is really key. Communication, as much as people say is important, becomes critical. It will make or break the relationship. I also found it helpful to schedule dates that I didn’t move and to try and check in about once a week with my girlfriend to see how the relationship was going. Everything was on the table and each person had the space to bring up anything they wanted, good or bad.\nAll of this being said, both people need to be willing to work through difficult times. The honeymoon phase wears off. Surprise assignments come up. I once was spending time with a girl I was dating and we were eating dinner together after a long week when someone on a group project texted me saying that he couldn’t finish the code. He had a week and had written three lines. It was due that night. So I had to finish it myself. Having a partner that understands that things like that come up is really important, but also as equally important is being able to communicate openly and honestly with each other if it ever becomes too much."
  },
  {
    "objectID": "posts/2024-06-05-undergrad-advice/index.html#values",
    "href": "posts/2024-06-05-undergrad-advice/index.html#values",
    "title": "Advice for Undergrads",
    "section": "Values",
    "text": "Values\nBecause the program is so rigorous, you’ll quickly learn the value of hard work, dedication, not giving up (both on yourself and others), thinking through problems, and learning how to balance life and school (as best as you can, at least). The value placed on hard work is one I’ve found to be specific in STEM, although it could definitely be present in other majors. Because everyone works so hard, you will go through hell and back multiple times with your friends. And you’ll all be stronger because of it and have a deeper relationship with each other. Those people will go to the ends of the earth for you, and I know that for the friends I have made through this program, if I were to need any of them for any kind of emergency, they would drop everything and help me. And they know I would do the same for them. No questions asked."
  },
  {
    "objectID": "posts/2024-06-07-gpt-chill/index.html",
    "href": "posts/2024-06-07-gpt-chill/index.html",
    "title": "GPT and Chill Notes",
    "section": "",
    "text": "Let’s assume that we have a model that takes two inputs: \\(x_1\\) (your weight at age 10) and \\(x_2\\) (your height at age 10) and outputs a prediction \\(y\\) for your final height as an adult.\nGradient descent is just a way to minimize a function. That’s it. So if I have a function like \\(y = x^2\\), we can minimize this pretty easily by taking the derivative and setting it equal to zero:\n\\[\n\\begin{align*}\ny &= x^2\\\\\ny' &= 2x\\\\\n2x &= 0\\\\\nx &= 0\n\\end{align*}\n\\]\nwhich corresponds to the minimum of \\(y = x^2\\). But that’s only for one input and one output. We usually have multiple inputs (like here it’s height and weight). So how can we handle that? What if we can’t even take the derivative?\nLet’s back up and ask a more important question first. Why do we even need to minimize a function to train a model?\nThe function we’re minimizing is the error function. Aka the loss or the cost function, it is the error between the model’s prediction and the label (the actual value). In our example here, the error would be the difference between what the model thinks the final height should be given a 10 year-old’s weight (\\(x_1\\)) and height (\\(x_2\\)), and what the height actually is (the label). There are different ways we can calculate the error, but here we could do something simple like the absolute difference:\n\\[\n\\begin{equation*}\n\\text{Error} \\approx \\left| \\text{Prediction} - \\text{label} \\right|\n\\end{equation*}\n\\] Let’s build some intuition. Let’s return to \\(y = x^2\\). Gradient descent is typically called “stochastic gradient descent” (SGD), with “stochastic” referring to a random guess for the minimum. Let’s apply that here to \\(y = x^2\\). Let’s have an initial guess of \\(x=3\\) (remember that we know the actual minimum is \\(x = 0\\)). Let’s look at the slope of the tangent line at \\(x = 3\\), which is the same thing as the derivative or the gradient. Here it is positive:\n\\[\n\\begin{align*}\nf' &= 2 x\\\\\nf' (x = 3) &= (2)(3) = 6\n\\end{align*}\n\\]\nwhich means that the function is increasing. If we want to minimize a function, we want to go in the opposite direction. We want to find where there is no change (\\(f'=0\\)), assuming the boundaries of the function do not contain the minima (that’s an easy check though). So our new guess:\n\\[\n\\text{New guess} = \\text{old guess} - \\left( \\text{slope} \\right) \\left( \\text{step size} \\right)\n\\] Let’s call the step size \\(\\alpha\\). So more succinctly:\n\\[\n\\text{Guess} -= \\text{slope} \\cdot \\alpha\n\\] In Python, we could write a class that implements this:\nclass GradDescent:\n    def get_minimizer(self, iterations: int, learning_rate: float, guess: int) -&gt; float:\n        for i in range(0, iterations):\n            guess -= (2*guess)*learning_rate\n\n        x = round(guess, 5)\n        \n        return x\n\nsol = GradDescent()\n\nx = sol.get_minimizer(iterations=10, learning_rate=0.01, guess=5)\n\nprint(x)\n\n\nLet’s say that we have a new function related with two variables:\n\\[\nf(x,y) = x^2 + y^2\n\\] Now we have an actual gradient, that is\n\\[\n\\begin{align*}\n\\nabla f(x,y) &= \\left \\langle \\frac{\\partial f}{ \\partial x }, \\frac{\\partial f}{ \\partial y } \\right \\rangle\\\\\n&= \\left \\langle 2x, 2y \\right \\rangle\n\\end{align*}\n\\] So now we have two values to update (one for each component, \\(x\\) and \\(y\\)), which means that we also need two initial guesses for \\(x\\) and \\(y\\). So our updating algorithm now looks like this for \\(x\\):\n\\[\n\\begin{align*}\n\\text{guess} &-= \\frac{\\partial f}{ \\partial x } \\bigg|_{x=x_{\\text{guess}}} \\cdot \\alpha\\\\\n&-= 2 x \\bigg|_{x=x_{\\text{guess}}} \\alpha\n\\end{align*}\n\\] and for \\(y\\): \\[\n\\begin{align*}\n\\text{guess} &-= \\frac{\\partial f}{ \\partial y } \\bigg|_{y=y_{\\text{guess}}} \\cdot \\alpha\\\\\n&-= 2 y \\bigg|_{y=y_{\\text{guess}}} \\alpha\n\\end{align*}\n\\] Note that the partial derivatives being the same here is purely a coincidence and is only because \\(f(x,y)\\) is defined as \\(f(x, y)=x^2 + y^2\\). If we had changed it to something like \\(f(x, y) = x^3 - y^{1/2}\\) or something like that, then the partial derivatives would obviously not be equal.\nIf you do the calculus, you find that the minimum of \\(f(x,y)\\) is at (0, 0). Here’s the code verifying that:\nclass GradDescentMulti:\n    def get_min(self, iterations: int, learning_rate: float, guess_x: int, guess_y: int) -&gt; float:\n        for _ in range(0, iterations):\n            guess_x -= 2*guess_x * learning_rate\n            guess_y -= 2*guess_y * learning_rate\n\n            guess_x = round(guess_x, 5)\n            guess_y = round(guess_y, 5)\n        return guess_x, guess_y\n\ngd = GradDescentMulti()\n\nx_min, y_min = gd.get_min(iterations=1000, learning_rate=0.1, guess_x = 2, guess_y = 5)\n\nprint(f'x_min: {x_min}, y_min: {y_min}')\nx_min: 2e-05, y_min: 2e-05\nwhich within rounding precision is equal to zero for both \\(x\\) and \\(y\\)."
  },
  {
    "objectID": "posts/2024-06-07-gpt-chill/vid1_scripts/grad_descent.html",
    "href": "posts/2024-06-07-gpt-chill/vid1_scripts/grad_descent.html",
    "title": "",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pt\n\n\n\n# Initialize function as y = x^2\ndef fxn(x):\n    return x**2\n\n\n# Initialize derivative of y, y' = 2*x\ndef deriv(x):\n    return 2*x    \n\n\n# Random guess\nguess = 3\n\n# Step size\nstep_size = 0.1\nalpha = step_size\n\n# Number of desired iterations\nnum_iterations = 30\n\n\nguesses = []\n\nfor i in range(0, num_iterations):\n    d = deriv(x=guess)\n    guess -= d*alpha\n\n    guesses.append(guess)\n\nGenerate plot of \\(y=x^2\\) and the corresponding guesses\n\nx_plot = np.linspace(start=-10, stop=10, num=1000)\ny_plot = fxn(x_plot)\n\ny_guesses = fxn(np.array(guesses))\n\n\nplt.plot(x_plot, y_plot, 'k-')\nplt.plot(guesses, y_guesses, 'ro', markerfacecolor='none', label='Guesses')\nplt.grid(True)\nplt.title('Guess of x=3')\nplt.show()\n\n\n\n\n\n\n\n\nThis also works for a different guess, like \\(x=-5\\):\n\n# Random guess\nguess = -5\n\n# Step size\nstep_size = 0.1\nalpha = step_size\n\n# Number of desired iterations\nnum_iterations = 30\n\n\nguesses = []\n\nfor i in range(0, num_iterations):\n    d = deriv(x=guess)\n    guess -= d*alpha\n\n    guesses.append(guess)\n\n\nx_plot = np.linspace(start=-10, stop=10, num=1000)\ny_plot = fxn(x_plot)\n\ny_guesses = fxn(np.array(guesses))\n\n\nplt.plot(x_plot, y_plot, 'k-')\nplt.plot(guesses, y_guesses, 'ro', markerfacecolor='none', label='Guesses')\nplt.grid(True)\nplt.title('Guess of x=-5')\nplt.show()\n\n\n\n\n\n\n\n\n\nclass GradDescent:\n    def get_minimizer(self, iterations: int, learning_rate: float, guess: int) -&gt; float:\n        for i in range(0, iterations):\n            guess -= (2*guess)*learning_rate\n\n        x = round(guess, 5)\n        \n        return x\n\nsol = GradDescent()\n\nx = sol.get_minimizer(iterations=10, learning_rate=0.01, guess=5)\n\nprint(x)\n\n4.08536\n\n\n\nclass GradDescentMulti:\n    def get_min(self, iterations: int, learning_rate: float, guess_x: int, guess_y: int) -&gt; float:\n        for _ in range(0, iterations):\n            guess_x -= 2*guess_x * learning_rate\n            guess_y -= 2*guess_y * learning_rate\n\n            guess_x = round(guess_x, 5)\n            guess_y = round(guess_y, 5)\n        return guess_x, guess_y\n\n\ngd = GradDescentMulti()\n\nx_min, y_min = gd.get_min(iterations=1000, learning_rate=0.1, guess_x = 2, guess_y = 5)\n\nprint(f'x_min: {x_min}, y_min: {y_min}')\n\nx_min: 2e-05, y_min: 2e-05"
  },
  {
    "objectID": "posts/2024-06-07-gpt-chill/index.html#multivariate-gradient-descent",
    "href": "posts/2024-06-07-gpt-chill/index.html#multivariate-gradient-descent",
    "title": "GPT and Chill Notes",
    "section": "",
    "text": "Let’s say that we have a new function related with two variables:\n\\[\nf(x,y) = x^2 + y^2\n\\] Now we have an actual gradient, that is\n\\[\n\\begin{align*}\n\\nabla f(x,y) &= \\left \\langle \\frac{\\partial f}{ \\partial x }, \\frac{\\partial f}{ \\partial y } \\right \\rangle\\\\\n&= \\left \\langle 2x, 2y \\right \\rangle\n\\end{align*}\n\\] So now we have two values to update (one for each component, \\(x\\) and \\(y\\)), which means that we also need two initial guesses for \\(x\\) and \\(y\\). So our updating algorithm now looks like this for \\(x\\):\n\\[\n\\begin{align*}\n\\text{guess} &-= \\frac{\\partial f}{ \\partial x } \\bigg|_{x=x_{\\text{guess}}} \\cdot \\alpha\\\\\n&-= 2 x \\bigg|_{x=x_{\\text{guess}}} \\alpha\n\\end{align*}\n\\] and for \\(y\\): \\[\n\\begin{align*}\n\\text{guess} &-= \\frac{\\partial f}{ \\partial y } \\bigg|_{y=y_{\\text{guess}}} \\cdot \\alpha\\\\\n&-= 2 y \\bigg|_{y=y_{\\text{guess}}} \\alpha\n\\end{align*}\n\\] Note that the partial derivatives being the same here is purely a coincidence and is only because \\(f(x,y)\\) is defined as \\(f(x, y)=x^2 + y^2\\). If we had changed it to something like \\(f(x, y) = x^3 - y^{1/2}\\) or something like that, then the partial derivatives would obviously not be equal.\nIf you do the calculus, you find that the minimum of \\(f(x,y)\\) is at (0, 0). Here’s the code verifying that:\nclass GradDescentMulti:\n    def get_min(self, iterations: int, learning_rate: float, guess_x: int, guess_y: int) -&gt; float:\n        for _ in range(0, iterations):\n            guess_x -= 2*guess_x * learning_rate\n            guess_y -= 2*guess_y * learning_rate\n\n            guess_x = round(guess_x, 5)\n            guess_y = round(guess_y, 5)\n        return guess_x, guess_y\n\ngd = GradDescentMulti()\n\nx_min, y_min = gd.get_min(iterations=1000, learning_rate=0.1, guess_x = 2, guess_y = 5)\n\nprint(f'x_min: {x_min}, y_min: {y_min}')\nx_min: 2e-05, y_min: 2e-05\nwhich within rounding precision is equal to zero for both \\(x\\) and \\(y\\)."
  },
  {
    "objectID": "posts/2024-06-07-gpt-chill/index.html#the-regression",
    "href": "posts/2024-06-07-gpt-chill/index.html#the-regression",
    "title": "GPT and Chill Notes",
    "section": "The “Regression”",
    "text": "The “Regression”"
  },
  {
    "objectID": "posts/2024-06-07-gpt-chill/index.html#the-regression-part",
    "href": "posts/2024-06-07-gpt-chill/index.html#the-regression-part",
    "title": "GPT and Chill Notes",
    "section": "The “Regression” part",
    "text": "The “Regression” part\n\nClassification\nLet’s start with a more intuitive example, which is actually the opposite of regression and it’s classification. As an example, let’s say that we’re building a model that detects whether someone has diabetes. There are two (simplified) outcomes here: the person has diabetes or they don’t. So we’re classifying our input (maybe an image) into two classification “buckets”.\nA more complicated example would be an object-detection model. So you give your model an input image and it tells you whether it’s a cat, a dog, an apple, or an orange. This is the same as the diabetes example, just with more classification buckets.\n\n\nRegression\nFor regression, the output is a number that is real (between negative and positive infinity). So if you want a model that will predict someone’s final height given their current height, current weight, how tall their parent’s are (and possibly any other relevant features), the output exists on some scale (so a number). Unlike classification, regression returns a number (for our example, something like 70, for 70 inches in height which is 5’10”) instead of a category (like “orange”)."
  },
  {
    "objectID": "posts/2024-06-07-gpt-chill/index.html#the-linear-part",
    "href": "posts/2024-06-07-gpt-chill/index.html#the-linear-part",
    "title": "GPT and Chill Notes",
    "section": "The “Linear” part",
    "text": "The “Linear” part\nWhen we’re building our model, we want it to make some kind of prediction given some piece of information we provide. The linear part here is saying that the relationship between the input to the model (like the current height, current weight, parent’s height, etc.) and the output (final height) is linear, so it looks something like this:\n\\[\nh(x, y, z) = w_1 x + w_2 y + w_3 z + b\n\\] where\n\\[\n\\begin{align*}\nh &\\text{ is the final height}\\\\\nx &\\text{ is the current weight}\\\\\ny &\\text{ is the current height}\\\\\nz &\\text{ is the parents' height}\\\\\nw_n &\\text{ is the model weight}\\\\\nb &\\text{ is the model bias}\\\\\n\\end{align*}\n\\] The “linear” part is from \\(h(x, y, z)\\) being a linear equation. So there isn’t anythng like \\(w_1 x^4\\) or \\(w_2 \\cos{y}\\). This means that, in short, we have a really fancy version of \\(y = mx + b\\), but \\(m\\) and \\(x\\) can have an arbitrary number of components or inputs.\nThen, during training via gradient descent, the model will improve \\(w_1, w_2, w_3, \\ldots, w_n\\) and \\(b\\) over some fixed number of iterations until the model is a pretty good way to predict someone’s final height given their current height, weight, and that of their parents.\nThe pseudocode for this would look something like:\nfor num_iterations:\n  get_model_prediction()\n  get_error() # want this to approach zero\n  get_derivatives()\n  update_weights()\nFocusing a bit more on the get_error() part, there are many different ways we can determine the error in a model, but one of the most common ones is called Mean Squared Error (MSE):\n\\[\n\\text{MSE} = \\sum_{i = 1}^{N} \\frac{\\left( \\text{prediction}_i - \\text{label}_i \\right)^2}{N}\n\\] where \\(N\\) is the number of training examples, and recall that the label is defined as the “true” answer that we’re comparing our model’s prediction against.\nA common question to ask here is why don’t we use the absolute value instead of squaring to get our error? This is because the derivative won’t exist somewhere. If we look at the most basic absolute value function, \\(y = |x|\\), we get something like this:\n\n\n\n\n\n\n\n\n\nwhich has an undefined derivative at the origin (technically, this makes the absolute value function “non-differentiable”). In other words, for \\(y = |x|\\), \\(y'\\) doesn’t exist at \\(x=0\\), which would break the algorithm that we’ve developed so far. Squaring solves this problem and still gives us the same general idea as the absolute value, that is, tells us how good/bad the error is.\nImplementing this in code, we use vectors and matrices:\n\\[\n\\begin{bmatrix}\nx & y & z\n\\end{bmatrix}\n\\begin{bmatrix}\nw_1\\\\\nw_2\\\\\nw_3\n\\end{bmatrix}\n= w_1 x + w_2 y + w_3 z = \\text{model prediction}\n\\] But what if we had many people? Let’s look at what this would look like if we had three people: \\[\n\\begin{bmatrix}\nx_1 & y_1 & z_1\\\\\nx_2 & y_2 & z_2\\\\\nx_3 & y_3 & z_3\n\\end{bmatrix}\n\\begin{bmatrix}\nw_1\\\\\nw_2\\\\\nw_3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nw_1 x_1 + w_2 y_1 + w_3 z_1\\\\\nw_1 x_2 + w_2 y_2 + w_3 z_2\\\\\nw_1 x_3 + w_2 y_3 + w_3 z_3\\\\\n\\end{bmatrix}\n\\] where each row is the model’s prediction for each person (i.e. the first row is the model’s prediction for the first person, second row is the prediction for the second person, etc.). An important note here is that at this point we’re just doing matrix multiplication. The main advantage of doing this in matrices vs. a loop is the following:\n\nPrograms can do this EXTREMELY fast"
  },
  {
    "objectID": "posts/2024-06-07-gpt-chill/index.html#implementation",
    "href": "posts/2024-06-07-gpt-chill/index.html#implementation",
    "title": "GPT and Chill Notes",
    "section": "Implementation",
    "text": "Implementation\n\nSolution - First Problem\nMy solution to the first coding problem:\nimport numpy as np\nfrom numpy.typing import NDArray\n\n\n# Helpful functions:\n# https://numpy.org/doc/stable/reference/generated/numpy.matmul.html\n# https://numpy.org/doc/stable/reference/generated/numpy.mean.html\n# https://numpy.org/doc/stable/reference/generated/numpy.square.html\n\nclass Solution:\n    \n    def get_model_prediction(self, X: NDArray[np.float64], weights: NDArray[np.float64]) -&gt; NDArray[np.float64]:\n        pred = np.matmul(X, weights)\n\n        return np.round(pred, 5)\n\n        # X is an Nx3 NumPy array\n        # weights is a 3x1 NumPy array\n        # HINT: np.matmul() will be useful\n        # return np.round(your_answer, 5)\n\n\n    def get_error(self, model_prediction: NDArray[np.float64], ground_truth: NDArray[np.float64]) -&gt; float:\n\n        error = np.mean(np.square(model_prediction - ground_truth))\n\n        return round(error, 5)\n\n        # model_prediction is an Nx1 NumPy array\n        # ground_truth is an Nx1 NumPy array\n        # HINT: np.mean(), np.square() will be useful\n        # return round(your_answer, 5)\n\n\nSolution - Second Problem\nimport numpy as np\nfrom numpy.typing import NDArray\n\n\nclass Solution:\n    def get_derivative(self, model_prediction: NDArray[np.float64], ground_truth: NDArray[np.float64], N: int, X: NDArray[np.float64], desired_weight: int) -&gt; float:\n        # note that N is just len(X)\n        return -2 * np.dot(ground_truth - model_prediction, X[:, desired_weight]) / N\n\n    def get_model_prediction(self, X: NDArray[np.float64], weights: NDArray[np.float64]) -&gt; NDArray[np.float64]:\n        return np.squeeze(np.matmul(X, weights))\n\n    learning_rate = 0.01\n\n    def train_model(\n        self, \n        X: NDArray[np.float64], \n        Y: NDArray[np.float64], \n        num_iterations: int, \n        initial_weights: NDArray[np.float64]\n    ) -&gt; NDArray[np.float64]:\n\n        for _ in range(num_iterations):\n            model_pred = self.get_model_prediction(X, initial_weights)\n\n            d1 = self.get_derivative(model_pred, Y, len(X), X, 0)\n            d2 = self.get_derivative(model_pred, Y, len(X), X, 1)\n            d3 = self.get_derivative(model_pred, Y, len(X), X, 2)\n\n            initial_weights[0] = initial_weights[0] - d1 * self.learning_rate\n            initial_weights[1] = initial_weights[1] - d2 * self.learning_rate\n            initial_weights[2] = initial_weights[2] - d3 * self.learning_rate\n\n        return np.round(initial_weights, 5)\nI initially struggled with this problem due to some of the syntax and eventually had to look at the solution because I was so stuck. I was quite close in my solution (basing the derivative section off of what I did for multivariate gradient descent), however I originally thought that desired_weight was what we wanted the weights to be at the end of training, which was confusing. It’s really just pointing to which weight we want to update (so for \\(w_1\\), desired_weight=0. for \\(w_2\\), desired_weight=1, etc.)."
  },
  {
    "objectID": "posts/2024-06-07-gpt-chill/index.html#linear-regression",
    "href": "posts/2024-06-07-gpt-chill/index.html#linear-regression",
    "title": "GPT and Chill Notes",
    "section": "Linear Regression",
    "text": "Linear Regression\nNeed linear regression to explain the connections and the hidden layer. Here’s the formula again:\n\\[\ny = w_1 x_1 + w_2 x_2 + w_3 x_3 + b\n\\] Remember that \\(x_n\\) are the input attributes and \\(y\\) is the model’s output/prediction. The weights \\(w_n\\) then determine the relative importance of each input parameter they are associated with. For example, \\(w_1\\) determines how important \\(x_1\\) (in this case the average parents’ height) is on someone’s final height (\\(y\\)).\nWhat about the bias?\nIn this context, it would probably be some base height, since a final height of zero doesn’t make much sense."
  },
  {
    "objectID": "posts/2024-06-07-gpt-chill/index.html#hidden-layer",
    "href": "posts/2024-06-07-gpt-chill/index.html#hidden-layer",
    "title": "GPT and Chill Notes",
    "section": "Hidden Layer",
    "text": "Hidden Layer\nEach of the hidden nodes is doing linear regression. Each node is calculating some \\(y\\) (\\(y_1\\), \\(y_2\\), \\(y_3\\), …) based on \\(x_1\\), \\(x_2\\), and \\(x_3\\) from the previous layer. This means that each node in the hidden layer (since it is calculating a \\(y\\)) is learning a \\(w_1\\), \\(w_2\\), and \\(w_3\\) independently of the other nodes in that same layer (i.e. \\(y_1 \\neq y_2\\))."
  },
  {
    "objectID": "posts/2024-06-07-gpt-chill/index.html#output-layer",
    "href": "posts/2024-06-07-gpt-chill/index.html#output-layer",
    "title": "GPT and Chill Notes",
    "section": "Output Layer",
    "text": "Output Layer\nWe can think of the two outputs as \\(O_1\\) and \\(O_2\\). Just as in the previous layer, \\(O_1\\) and \\(O_2\\) are also calculated using a linear regression formula:\n\\[\n\\begin{align*}\nO_1 &= w_1 y_1 + w_2 y_2 + w_3 y_3 + w_4 y_4 + b\\\\\nO_2 &= w_1 y_1 + w_2 y_2 + w_3 y_3 + w_4 y_4 + b\\\\\n\\end{align*}\n\\]\nBUT the difference here is that the \\(w\\)’s and the \\(b\\) for \\(O_1\\) are learned independently from \\(O_2\\) and are going to be different.\nIf the point of the model is to predict someone’s final height, that final prediction may be from averaging \\(O_1\\) and \\(O_2\\). Training then is figuring out what \\(w_1\\), \\(w_2\\), \\(w_3\\), \\(w_4\\), and \\(b\\) should be for \\(y_1\\) - \\(y_4\\) such that the error is minimized. The same thing is done for \\(O_1\\) and \\(O_2\\)."
  },
  {
    "objectID": "posts/2024-06-07-gpt-chill/scripts/pytorch_intro.html",
    "href": "posts/2024-06-07-gpt-chill/scripts/pytorch_intro.html",
    "title": "Video 8: Introduction to Pytorch",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nPyTorch might be the only library that is needed. Might need some other libraries like NumPy or Pandas, but can usually get away with only using PyTorch. Fundamental data type here is the Tensor. PyTorch (and the Tensor data type more specifically) will also take care of all the ugly math, like nasty derivatives and huge matrix multiplications.\na = torch.ones(5, 5)\n\nprint(a)\n\ntensor([[1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.]])\nFirst two important functions in PyTorch is sum() and mean():\nsum = torch.sum(a, axis=1) # Note that axis=1 corresponds to the rows, NOT the columns\n\n# Print the sum of each column\nprint(sum)\n\ntensor([5., 5., 5., 5., 5.])\nAlso have squeeze() and unsqueeze(), which is used when we have unnecessary dimensions\na = torch.ones(5, 1)\n\nprint(a.shape)\n\ntorch.Size([5, 1])\nBut the 1 is kind of unnecessary. Saying 5 x 1 is kind of unnecessary. So if we want to get rid of the 1 we can use the squeeze() method:\nprint(a.shape)\n\nsqueezed = torch.squeeze(a)\n\nprint(squeezed.shape)\n\ntorch.Size([5, 1])\ntorch.Size([5])\nWhile this may seem like a small difference, it will have an impact on functions later on. Here’s the difference in a more visual way:\nprint(a)\n\nprint(squeezed)\n\ntensor([[1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.]])\ntensor([1., 1., 1., 1., 1.])\nIf we can squeeze something, we should also have some way to unsqueeze as well. If we’re passing in two tensors, like one for training and one for validation, they both need to be the same size. We can’t have one look like a and one look like squeezed.\nHere, the argument dim is where we want to insert that extra dimension. So to make our tensor unsqueezed 5x1 instead of just 5, we pass dim=1:\nunsqueezed = torch.unsqueeze(squeezed, dim=1)\nprint(unsqueezed.shape)\nprint(unsqueezed)\n\ntorch.Size([5, 1])\ntensor([[1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.]])\nIf instead we pass dim=0, we get the opposite:\nunsqueezed = torch.unsqueeze(squeezed, dim=0)\nprint(unsqueezed.shape)\nprint(unsqueezed)\n\ntorch.Size([1, 5])\ntensor([[1., 1., 1., 1., 1.]])"
  },
  {
    "objectID": "posts/2024-06-07-gpt-chill/scripts/pytorch_intro.html#defining",
    "href": "posts/2024-06-07-gpt-chill/scripts/pytorch_intro.html#defining",
    "title": "Video 8: Introduction to Pytorch",
    "section": "Defining",
    "text": "Defining"
  },
  {
    "objectID": "posts/2024-06-07-gpt-chill/scripts/pytorch_intro.html#defining-neural-network-models-in-pytorch",
    "href": "posts/2024-06-07-gpt-chill/scripts/pytorch_intro.html#defining-neural-network-models-in-pytorch",
    "title": "Video 8: Introduction to Pytorch",
    "section": "Defining Neural Network Models in PyTorch",
    "text": "Defining Neural Network Models in PyTorch\n\n# class MyModel:\n    \n#     # Constructor - layers\n\n#     # Forward pass - get_model_prediction(example_datapoint)\n\n\n\nModule Class\nPyTorch has something like this, it’s called the Module class. A Module is basically the same thing as a model. Every model we create in PyTorch is going to inherit or subclass torch.nn.Module. Let’s make our own.\n\n\nLinear Class\nRemember that each layer in a neural network contains a bunch of nodes that are just doing linear regression based on the previous layer’s input attributes, so we need some class that can take in the previous layer’s input attributes, do linear regression, and then output them. That is what the Linear class does.\n\n\nExample\nIf you just Google “Neural network”, you might find something like this:\ninsert neural_network.svg, source is https://victorzhou.com/series/neural-networks-from-scratch/\nLet’s build it!\nLooks like our input layer has four nodes, and we have two hidden layers and one output layer. So we’re going to have three instances of nn.Linear.\n\nclass MyModel(nn.Module):\n    # Create constructor\n    def __init__(self):\n        super().__init__()\n        self.first_layer = nn.Linear(in_features=4, out_features=6)\n        self.second_layer = nn.Linear(in_features=6, out_features=6)\n        self.final_layer = nn.Linear(in_features=6, out_features=2)\n\n    # Create forward pass\n    def forward(self, x):\n        # Calling the forward method from the nn.Module class\n    #  first_layer_output = self.first_layer.forward(x)\n       # Can also do the following:\n       #  first_layer_output = self.first_layer(x)\n\n        return self.final_layer(self.second_layer(self.first_layer.forward(x)))\n\nWe still need to take care of the weights somehow. They’re actually hidden inside of the\n\nmodel = MyModel()\n\nexample_datapoint = torch.randn(1, 4)\n\nmodel.forward(x=example_datapoint) # Can also write model(x=example_datapoint)\n\ntensor([[-0.4633, -0.0682]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nThen we need to train the model for some number of iterations. We can then actually use the model to get predictions."
  },
  {
    "objectID": "posts/2024-06-07-gpt-chill/index.html#defining-neural-network-models-in-pytorch",
    "href": "posts/2024-06-07-gpt-chill/index.html#defining-neural-network-models-in-pytorch",
    "title": "GPT and Chill Notes",
    "section": "Defining Neural Network Models in PyTorch",
    "text": "Defining Neural Network Models in PyTorch\nWe want something that looks like this:\nclass MyModel:\n    # Constructor - layers\n\n    # Forward pass - get_model_prediction(example_datapoint)\n\nModule Class\nPyTorch has something like this, it’s called the Module class. A Module is basically the same thing as a model. Every model we create in PyTorch is going to inherit or subclass torch.nn.Module.\n\n\nLinear Class\nRemember that each layer in a neural network contains a bunch of nodes that are just doing linear regression based on the previous layer’s input attributes, so we need some class that can take in the previous layer’s input attributes, do linear regression, and then output them. That is what the Linear class does.\n\n\nExample\nIf you just Google “Neural network”, you might find something like this:\n\n\n\nNeural network architecture. Source\n\n\nLet’s build it!\nLooks like our input layer has four nodes, and we have two hidden layers and one output layer. So we’re going to have three instances of nn.Linear.\nclass MyModel(nn.Module):\n    # Create constructor\n    def __init__(self):\n        super().__init__()\n        self.first_layer = nn.Linear(in_features=4, out_features=6)\n        self.second_layer = nn.Linear(in_features=6, out_features=6)\n        self.final_layer = nn.Linear(in_features=6, out_features=2)\n\n    # Create forward pass\n    def forward(self, x):\n        # Calling the forward method from the nn.Module class\n    #  first_layer_output = self.first_layer.forward(x)\n       # Can also do the following:\n       #  first_layer_output = self.first_layer(x)\n\n        return self.final_layer(self.second_layer(self.first_layer.forward(x)))\n&gt;&gt;&gt; model = MyModel()\n&gt;&gt;&gt; example_datapoint = torch.randn(1, 4)\n&gt;&gt;&gt; model.forward(x=example_datapoint) # Can also write model(x=example_datapoint)\n\ntensor([[-0.4633, -0.0682]], grad_fn=&lt;AddmmBackward0&gt;)\nThen we need to train the model for some number of iterations. We can then actually use the model to get predictions."
  },
  {
    "objectID": "posts/2024-06-07-gpt-chill/index.html#coding-problem-solution",
    "href": "posts/2024-06-07-gpt-chill/index.html#coding-problem-solution",
    "title": "GPT and Chill Notes",
    "section": "Coding Problem Solution",
    "text": "Coding Problem Solution\nimport torch\nimport torch.nn\nfrom torchtyping import TensorType\n\n# Helpful functions:\n# https://pytorch.org/docs/stable/generated/torch.reshape.html\n# https://pytorch.org/docs/stable/generated/torch.mean.html\n# https://pytorch.org/docs/stable/generated/torch.cat.html\n# https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html\n\n# Round your answers to 4 decimal places using torch.round(input_tensor, decimals = 4)\nclass Solution:\n    def reshape(self, to_reshape: TensorType[float]) -&gt; TensorType[float]:\n    \n        return torch.reshape(to_reshape, (to_reshape.size(dim=1)*to_reshape.size(dim=0)//2, 2))\n\n\n    def average(self, to_avg: TensorType[float]) -&gt; TensorType[float]:\n        \n        return torch.mean(to_avg, axis=0)\n\n    def concatenate(self, cat_one: TensorType[float], cat_two: TensorType[float]) -&gt; TensorType[float]:\n\n        return torch.cat((cat_one, cat_two), dim=1)\n\n    def get_loss(self, prediction: TensorType[float], target: TensorType[float]) -&gt; TensorType[float]:\n\n        return torch.nn.functional.mse_loss(input=prediction, target=target)"
  },
  {
    "objectID": "scripts/pytorch_intro.html",
    "href": "scripts/pytorch_intro.html",
    "title": "Video 8: Introduction to Pytorch",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nPyTorch might be the only library that is needed. Might need some other libraries like NumPy or Pandas, but can usually get away with only using PyTorch. Fundamental data type here is the Tensor. PyTorch (and the Tensor data type more specifically) will also take care of all the ugly math, like nasty derivatives and huge matrix multiplications.\na = torch.ones(5, 5)\n\nprint(a)\n\ntensor([[1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.]])\nFirst two important functions in PyTorch is sum() and mean():\nsum = torch.sum(a, axis=1) # Note that axis=1 corresponds to the rows, NOT the columns\n\n# Print the sum of each column\nprint(sum)\n\ntensor([5., 5., 5., 5., 5.])\nAlso have squeeze() and unsqueeze(), which is used when we have unnecessary dimensions\na = torch.ones(5, 1)\n\nprint(a.shape)\n\ntorch.Size([5, 1])\nBut the 1 is kind of unnecessary. Saying 5 x 1 is kind of unnecessary. So if we want to get rid of the 1 we can use the squeeze() method:\nprint(a.shape)\n\nsqueezed = torch.squeeze(a)\n\nprint(squeezed.shape)\n\ntorch.Size([5, 1])\ntorch.Size([5])\nWhile this may seem like a small difference, it will have an impact on functions later on. Here’s the difference in a more visual way:\nprint(a)\n\nprint(squeezed)\n\ntensor([[1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.]])\ntensor([1., 1., 1., 1., 1.])\nIf we can squeeze something, we should also have some way to unsqueeze as well. If we’re passing in two tensors, like one for training and one for validation, they both need to be the same size. We can’t have one look like a and one look like squeezed.\nHere, the argument dim is where we want to insert that extra dimension. So to make our tensor unsqueezed 5x1 instead of just 5, we pass dim=1:\nunsqueezed = torch.unsqueeze(squeezed, dim=1)\nprint(unsqueezed.shape)\nprint(unsqueezed)\n\ntorch.Size([5, 1])\ntensor([[1.],\n        [1.],\n        [1.],\n        [1.],\n        [1.]])\nIf instead we pass dim=0, we get the opposite:\nunsqueezed = torch.unsqueeze(squeezed, dim=0)\nprint(unsqueezed.shape)\nprint(unsqueezed)\n\ntorch.Size([1, 5])\ntensor([[1., 1., 1., 1., 1.]])"
  },
  {
    "objectID": "scripts/pytorch_intro.html#defining-neural-network-models-in-pytorch",
    "href": "scripts/pytorch_intro.html#defining-neural-network-models-in-pytorch",
    "title": "Video 8: Introduction to Pytorch",
    "section": "Defining Neural Network Models in PyTorch",
    "text": "Defining Neural Network Models in PyTorch\n\n# class MyModel:\n    \n#     # Constructor - layers\n\n#     # Forward pass - get_model_prediction(example_datapoint)\n\n\n\nModule Class\nPyTorch has something like this, it’s called the Module class. A Module is basically the same thing as a model. Every model we create in PyTorch is going to inherit or subclass torch.nn.Module. Let’s make our own.\n\n\nLinear Class\nRemember that each layer in a neural network contains a bunch of nodes that are just doing linear regression based on the previous layer’s input attributes, so we need some class that can take in the previous layer’s input attributes, do linear regression, and then output them. That is what the Linear class does.\n\n\nExample\nIf you just Google “Neural network”, you might find something like this:\ninsert neural_network.svg, source is https://victorzhou.com/series/neural-networks-from-scratch/\nLet’s build it!\nLooks like our input layer has four nodes, and we have two hidden layers and one output layer. So we’re going to have three instances of nn.Linear.\n\nclass MyModel(nn.Module):\n    # Create constructor\n    def __init__(self):\n        super().__init__()\n        self.first_layer = nn.Linear(in_features=4, out_features=6)\n        self.second_layer = nn.Linear(in_features=6, out_features=6)\n        self.final_layer = nn.Linear(in_features=6, out_features=2)\n\n    # Create forward pass\n    def forward(self, x):\n        # Calling the forward method from the nn.Module class\n    #  first_layer_output = self.first_layer.forward(x)\n       # Can also do the following:\n       #  first_layer_output = self.first_layer(x)\n\n        return self.final_layer(self.second_layer(self.first_layer.forward(x)))\n\nWe still need to take care of the weights somehow. They’re actually hidden inside of the\n\nmodel = MyModel()\n\nexample_datapoint = torch.randn(1, 4)\n\nmodel.forward(x=example_datapoint) # Can also write model(x=example_datapoint)\n\ntensor([[-0.4633, -0.0682]], grad_fn=&lt;AddmmBackward0&gt;)\n\n\nThen we need to train the model for some number of iterations. We can then actually use the model to get predictions."
  },
  {
    "objectID": "scripts/pytorch_train.html",
    "href": "scripts/pytorch_train.html",
    "title": "",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\n\ntorch.manual_seed(0)\n\n&lt;torch._C.Generator at 0x7fbf4ab3f070&gt;\n\n\n\nclass DigitRecognition(nn.Module):\n    def __init__(self):\n        super().__init__()\n        torch.manual_seed(0)\n        self.first_linear = nn.Linear(in_features=784, out_features=512)\n        self.relu = nn.ReLU()\n        self.drop = nn.Dropout(p=0.2)\n        self.second_linear = nn.Linear(in_features=512, out_features=10)\n        self.sigmoid = nn.Sigmoid()\n        # Define the architecture here\n    \n    def forward(self, images: float) -&gt; float:\n        torch.manual_seed(0)\n        out =  self.sigmoid(self.second_linear(self.drop(self.relu(self.first_linear(images)))))\n\n        return out\n\nHow do we actually update this model over many iterations? Here we’re going to look at the training loop. The following code is the same regardless of the neural network:\n\nmodel = DigitRecognition()\n\nloss_function = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\n\nepochs = 5\nfor epoch in range(epochs):\n    for images, labels in train_dataloader():\n        images = images.view(images.shape[0], 784)\n\n        # Training body\n        model_prediction = model(images)\n        optimizer.zero_grad()\n        loss.loss_function(model_prediction, labels)\n        loss.backward()\n        optimizer.step()\n\nNameError: name 'train_dataloader' is not defined\n\n\nLet’s break this down a bit:\nmodel = DigitRecognition()\nThis is instantiating our model. This could change depending on the task we are doing, but we still need to instantiate some kind of model.\nloss_function = nn.CrossEntropyLoss()\nThis is our loss function. Here it’s cross-entropy loss instead of MSE since we are doing a classification problem instead of a regression problem. Again, the loss function will vary depending on the application, but we still need one.\noptimizer = torch.optim.Adam(model.parameters())\nThis is an object in PyTorch that does gradient descent for us. What we need to pass in are the weights (parameters). Adam is gradient descent on steroids. It is still doing gradient descent (with a default learning rate), but is optimized to dynamically change the learning rate.\nepochs = 5\nAn epoch is a single pass through the entire dataset. So here, the model is passing through the entire training dataset 5 times. Too many epochs might result in overfitting, too few might mean the weights haven’t been updated enough.\nfor images, labels in train_dataloader():\ntrain_dataloader() is just a sequence of steps to take in the raw data and do any preprocessing on it before it is fed to the network (for images, this might be things like resizing, cropping, or random reflections).\nimages = images.view(images.shape[0], 784)\nCould have also used images = images.reshape(...), but we’re reshaping the image from 28 * 28 to a single vector with length 784 so that the network (which has a first layer with 784 neurons) can take it in.\nmodel_prediction = model(images)\nThis is calling the forward() method of the model class we defined earlier and gets our model predictions.\noptimizer.zero_grad()\nThis is a frustrating line that really shouldn’t be required in PyTorch, but it cancels out all of the derivatives that were calculated from the previous iteration of gradient descent. This is done because for each iteration, we want to calculate the derivatives again so that we can update our weights. This line resets the derivatives so we can do this.\nloss.loss_function(model_prediction, labels)\nThis is calculating the loss/error based on our current model’s prediction and the labels.\nloss.backward()\nPotentially the most important line here. This will calculate every single derivative necessary to perform gradient descent. This is the most computationally intensive step of this entire program. It is calculating all of the derivatives and is storing them in such a way that we can use them later.\noptimizer.step()\nThis updates all of the weights, so essentially doing \\(\\text{new weight} = \\text{old weight} - \\text{derivative} \\cdot \\text{learning rate}\\). Next chunk:\nmodel.eval()\n\nfor images, labels in test_dataloader:\n    images = images.view(images.shape[0], 784)\n\n    model_prediction = model(images)\n    max, idx = torch.max(model_prediction, dim=1)\n    for i in range(len(images)):\n        plt.imshow(images[i].view(28, 28))\n        plt.show()\n        print(idx[i].item())\n    break\nThis is showing us how well the model is doing once training is done.\nmodel.eval()\nThis puts the model in evaluation mode. We’re telling PyTorch that we just want the model’s predictions, so it doesn’t need to calculate any derivatives for training.\nThen we iterate over our test dataloader, reshape our images, pass them into the model, and get our predictions. For every image, we’re predicting a bunch of probabilities (specifically 10). So we should take the maximum probability and map that to the class. This lets us see the image and the corresponding digit the model thinks it is. Then iterate over each image and do this."
  },
  {
    "objectID": "posts/2024-06-07-gpt-chill/index.html#coding-problem-solution-1",
    "href": "posts/2024-06-07-gpt-chill/index.html#coding-problem-solution-1",
    "title": "GPT and Chill Notes",
    "section": "Coding Problem Solution",
    "text": "Coding Problem Solution\nThis was a very poorly written question. It should have been stated as the following:\nModel architecture takes in a 28*28 image and sends that to 512 neurons (use a linar layer for this). This is then followed by a ReLU activation function and then a dropout layer with probability p=0.2. Then there is a final linear layer that shrinks 512 neurons down to 10, followed finally by a sigmoid activation function.\nimport torch\nimport torch.nn as nn\nfrom torchtyping import TensorType\n\nclass Solution(nn.Module):\n    def __init__(self):\n        super().__init__()\n        torch.manual_seed(0)\n        self.first_linear = nn.Linear(in_features=784, out_features=512)\n        self.relu = nn.ReLU()\n        self.drop = nn.Dropout(p=0.2)\n        self.second_linear = nn.Linear(in_features=512, out_features=10)\n        self.sigmoid = nn.Sigmoid()\n        # Define the architecture here\n    \n    def forward(self, images: TensorType[float]) -&gt; TensorType[float]:\n        torch.manual_seed(0)\n        out =  self.sigmoid(self.second_linear(self.drop(self.relu(self.first_linear(images)))))\n\n        # Return the model's prediction to 4 decimal places\n        return torch.round(out, decimals=4)"
  }
]